{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Bagging & Random Forests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble Learning: Bootstrap Aggregation(Bagging) & Random Forests\n",
        "# **Introduction:**\n",
        "\n",
        "We already discussed decision trees in our tutorial about it in-depth. We noticed over there that a single decision tree generally over-fits train data very easily hence it's a better idea to combine many decision trees to make a decision. The basic idea is that multiple overfitting estimators can be combined together to reduce the effect of overfitting and produce better predictions which generalize well. This idea is generally referred to as ensemble learning in the machine learning community.\n",
        "There are 2 ways to combine decision trees to make better decisions:\n",
        "\n",
        " - **Averaging** (**Bootstrap Aggregation - Bagging & Random Forests**) - Idea is that we create many individual estimators and average predictions of these estimators to make the final predictions. Averaging estimators reduce variance hence avoids overfitting.\n",
        " - **Boosting** - Base estimators are trained sequentially where we try to reduce the bias of combined estimator hence avoid underfitting. The main idea is to combine a few weak estimators to create a powerful estimator.\n",
        "Fast to train and easy to understand & interpret.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "# **Importing necessary libraries** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import ensemble, datasets, tree\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"Python Version : \",sys.version)\n",
        "print(\"Scikit-Learn Version : \",sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN",
        "colab_type": "text"
      },
      "source": [
        "# 1 **Bootstrap Aggregation (Bagging)**\n",
        "\n",
        "Bagging starts with many sub-sample of original data with replacement and then trains various decision trees on these sub-samples. When the prediction is to be made on new data, it votes or averages prediction from each decision tree. The basic idea is to solve the overfitting problem (reducing high variance) by introducing some randomization.\n",
        "\n",
        "Scikit-Learn provides BagginRegressor and BaggingClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aCyhDgoTRK",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 BaggingRegressor \n",
        "We'll be explaining the usage of BaggingRegressor by using the Boston housing data set. We'll first train the model with default parameters and then do hyper-parameter tuning. We'll also be comparing the performance of tuned bagging estimator with decision tree and extra tree estimator of scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.1 - Loading Data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1wsGi52CAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "boston = datasets.load_boston()\n",
        "X_boston, Y_boston = boston.data, boston.target\n",
        "print('Dataset features names : '+str(boston.feature_names))\n",
        "print('Dataset features size : '+str(boston.data.shape))\n",
        "print('Dataset target size : '+str(boston.target.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.2 - Splitting Dataset into Train & Test sets ##\n",
        "We'll split the dataset into two parts:\n",
        " - Training data which will be used for the training model.\n",
        " - Test data against which accuracy of the trained model will be checked.\n",
        "train_test_split function of model_selection module of sklearn will help us split data into two sets with 80% for training and 20% for test purposes. We are also using seed(random_state=123) with train_test_split so that we always get the same split and can reproduce results in the future as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9JGZRKCCAcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_boston, Y_boston , train_size=0.80, test_size=0.20, random_state=123)\n",
        "print('Train/Test Sets Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.3- Fitting Model To Train Data \n",
        "We can fit() method on estimator passing it train features and train target. It'll then train a model using that data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "bag_regressor = BaggingRegressor(random_state=1)\n",
        "bag_regressor.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.4-Evaluating Trained Model On Test Data\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Y_preds = bag_regressor.predict(X_test)\n",
        "print(Y_preds[:10])\n",
        "print(Y_test[:10])\n",
        "print('Training Coefficient of R^2 : %.3f'%bag_regressor.score(X_train, Y_train))\n",
        "print('Test Coefficient of R^2 : %.3f'%bag_regressor.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.5 - Finetuning Model By Doing Grid Search On Various Hyperparameters\n",
        "\n",
        "Below are list of common hyperparameters which needs tuning for getting best fit for our data. We'll try various hyperparemters settings to various splits of train/test data to find out best fit which will have almost same accuracy for both train & test dataset or have quite less different between accuracy.\n",
        "\n",
        "  - base_estimator(object or None) - Base Estimator whose many instances will be created. If None is provided then DecisionTree wil be used as base estimator.It accepts object or None. default=None\n",
        "  - n_estimators(int) - Number of base estimators whose results will be combined to produce final prediction. default=10\n",
        "  - bootstrap(bool) - Decides whether samples are drawn with replacement. True = With Replacement. False = Without Replacement.default=True\n",
        "  - bootstrap_features(bool) - Decides whether features are drawn with replacement. True = With Replacement. False = Without Replacement.default=False\n",
        "  - max_samples(int/float) - It accepts int(1-n_samples) or float(0.0-1.0] values. It represents number of samples to draw from train data to train particular estimator.\n",
        "  - max_features(int/float) - It accepts int(1-n_features) or float(0.0-1.0] values. It represents number of features to draw from train data to train particular estimator.\n",
        "\n",
        "**GridSearchCV**\n",
        "\n",
        "It's a wrapper class provided by sklearn which loops through all parameters provided as params_grid parameter with a number of cross-validation folds provided as cv parameter, evaluates model performance on all combinations and stores all results in cv_results_ attribute. It also stores model which performs best in all cross-validation folds in best_estimator_ attribute and best score in best_score_ attribute.\n",
        "\n",
        "We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by doing 3-fold cross-validation on data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "n_samples = boston.data.shape[0]\n",
        "n_features = boston.data.shape[1]\n",
        "\n",
        "params = {'base_estimator': [None, LinearRegression(), KNeighborsRegressor()],\n",
        "          'n_estimators': [20,50,100],\n",
        "          'max_samples': [0.5,1.0, n_samples//2,],\n",
        "          'max_features': [0.5,1.0, n_features//2,],\n",
        "          'bootstrap': [True, False],\n",
        "          'bootstrap_features': [True, False]}\n",
        "\n",
        "bagging_regressor_grid = GridSearchCV(BaggingRegressor(random_state=1, n_jobs=-1), param_grid =params, cv=3, n_jobs=-1, verbose=1)\n",
        "bagging_regressor_grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test R^2 Score : %.3f'%bagging_regressor_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best R^2 Score Through Grid Search : %.3f'%bagging_regressor_grid.best_score_)\n",
        "print('Best Parameters : ',bagging_regressor_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g78HCuLVCAdG",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.6 - Printing First Few Cross-Validation Results \n",
        "GridSearchCV maintains results for all parameter combinations tried with all cross-validation splits. We can access results for all iterations as a dictionary by calling cv_results_ attribute on it. We are converting it to pandas dataframe for better visuals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTAYjq6JOpj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(bagging_regressor_grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg-zk8uciMg_",
        "colab_type": "text"
      },
      "source": [
        "### 1.1.7- Comparing Performance Of Bagging With Decision Tree/Extra Tree\n",
        "Below we are comparing the performance of various bagging regression estimators with a decision tree and extra tree estimators. We can notice that bagging estimators do not over-fit like a decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQLu4j6Iib4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  ## Bagging Regressor with Default Params\n",
        "bag_regressor = ensemble.BaggingRegressor(random_state=1)\n",
        "bag_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(bag_regressor.__class__.__name__,\n",
        "                                                     bag_regressor.score(X_train, Y_train),bag_regressor.score(X_test, Y_test)))\n",
        "\n",
        "## Bagging Regressor with KNeighborsRegressor as base estimator\n",
        "bag_regressor = ensemble.BaggingRegressor(base_estimator=KNeighborsRegressor(), random_state=1)\n",
        "bag_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(bag_regressor.__class__.__name__,\n",
        "                                                          bag_regressor.score(X_train, Y_train),bag_regressor.score(X_test, Y_test)))\n",
        "\n",
        "## Above Hyper-peramter tuned Bagging Regressor\n",
        "bag_regressor = ensemble.BaggingRegressor(random_state=1, **bagging_regressor_grid.best_params_)\n",
        "bag_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(bag_regressor.__class__.__name__,\n",
        "                                                     bag_regressor.score(X_train, Y_train),bag_regressor.score(X_test, Y_test)))\n",
        "\n",
        "## Decision Tree with Default Parameters\n",
        "dtree_regressor = tree.DecisionTreeRegressor(random_state=1)\n",
        "dtree_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(dtree_regressor.__class__.__name__,\n",
        "                                                     dtree_regressor.score(X_train, Y_train),dtree_regressor.score(X_test, Y_test)))\n",
        "\n",
        "## Decision Tree with Default Parameters\n",
        "extra_tree_regressor = tree.ExtraTreeRegressor(random_state=1)\n",
        "extra_tree_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_tree_regressor.__class__.__name__,\n",
        "                                                     extra_tree_regressor.score(X_train, Y_train),extra_tree_regressor.score(X_test, Y_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GIu_3jPi0Ed",
        "colab_type": "text"
      },
      "source": [
        "## 1.2- BaggingClassifier \n",
        "We'll be explaining the usage of BaggingClassifier by using digits data set. We'll first train the model with default parameters and then do hyper-parameter tuning. We'll also be comparing the performance of tuned bagging estimator with decision tree and extra tree estimator of scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT-VLB8xjfuj",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1 - Load DIGITS Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z3W2pYv-cTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "digits = datasets.load_digits()\n",
        "X_digits, Y_digits = digits.data, digits.target\n",
        "print('Dataset Size : ', X_digits.shape, Y_digits.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wexnQdb59tZT",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2 Splitting Dataset into Train & Test sets \n",
        "Below we are splitting the Boston dataset into train set(80%) and test set(20%). We are also using seed(random_state=123) so that we always get the same split and can reproduce results in the future as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luhIk0B-90rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size=0.80, test_size=0.20, stratify=Y_digits, random_state=123)\n",
        "print('Train/Test Set Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfhrppsrQ6pg",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.3 Fitting model on training Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewWWOuH-RGbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "bag_classifier = BaggingClassifier(random_state=1)\n",
        "bag_classifier.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRgocQiW9-dO",
        "colab_type": "text"
      },
      "source": [
        "###1.2.4 Evaluating Trained Model On Test Data ###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zQ035HR-PEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Y_preds = bag_classifier.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "\n",
        "print('Test Accuracy : %.3f'%(Y_preds == Y_test).mean())\n",
        "print('Test Accuracy : %.3f'%bag_classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%bag_classifier.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIphSupp-dJ2",
        "colab_type": "text"
      },
      "source": [
        "###1.2.5- Finetuning Model By Doing Grid Search On Various Hyperparameters###\n",
        "BaggingClassifier has the same parameters to tune as that of BaggingRegressor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg4jUe2v-qYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "n_samples = digits.data.shape[0]\n",
        "n_features = digits.data.shape[1]\n",
        "\n",
        "params = {'base_estimator': [None, LogisticRegression(), KNeighborsClassifier()],\n",
        "          'n_estimators': [20,50,100],\n",
        "          'max_samples': [0.5, 1.0, n_samples//2, ],\n",
        "          'max_features': [0.5, 1.0, n_features//2, ],\n",
        "          'bootstrap': [True, False],\n",
        "          'bootstrap_features': [True, False]}\n",
        "\n",
        "bagging_classifier_grid = GridSearchCV(BaggingClassifier(random_state=1, n_jobs=-1), param_grid =params, cv=3, n_jobs=-1, verbose=1)\n",
        "bagging_classifier_grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accurqacy : %.3f'%bagging_classifier_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%bagging_classifier_grid.best_score_)\n",
        "print('Best Parameters : ',bagging_classifier_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvO7qP_V-4oo",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.6 -Printing First Few Cross Validation Results###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRbBqF42_IYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(bagging_classifier_grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgLxzj33_PvP",
        "colab_type": "text"
      },
      "source": [
        "###1.2.7- Comparing Performance Of Bagging With Decision Tree/Extra Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgvLJ3YO_dYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bag_classifier = ensemble.BaggingClassifier(random_state=1)\n",
        "bag_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(bag_classifier.__class__.__name__,\n",
        "                                                     bag_classifier.score(X_train, Y_train),bag_classifier.score(X_test, Y_test)))\n",
        "\n",
        "bag_classifier = ensemble.BaggingClassifier(base_estimator=KNeighborsClassifier(), random_state=1)\n",
        "bag_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(bag_classifier.__class__.__name__,\n",
        "                                                     bag_classifier.score(X_train, Y_train),bag_classifier.score(X_test, Y_test)))\n",
        "\n",
        "bag_classifier = ensemble.BaggingClassifier(random_state=1, **bagging_classifier_grid.best_params_)\n",
        "bag_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(bag_classifier.__class__.__name__,\n",
        "                                                     bag_classifier.score(X_train, Y_train),bag_classifier.score(X_test, Y_test)))\n",
        "\n",
        "dtree_classifier = tree.DecisionTreeClassifier(random_state=1)\n",
        "dtree_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(dtree_classifier.__class__.__name__,\n",
        "                                                     dtree_classifier.score(X_train, Y_train),dtree_classifier.score(X_test, Y_test)))\n",
        "\n",
        "extra_tree_classifier = tree.ExtraTreeClassifier(random_state=1)\n",
        "extra_tree_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_tree_classifier.__class__.__name__,\n",
        "                                                     extra_tree_classifier.score(X_train, Y_train),extra_tree_classifier.score(X_test, Y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trbO8CCt_oMQ",
        "colab_type": "text"
      },
      "source": [
        "# 2 Random Forests \n",
        "Random Forests are slight improvements over bagging. Combining predictions from various decision trees works well when these decision trees predictions are as less correlated as possible. In a sense, each sub-tree is predicting some class of problem very well then all other sub-trees. The problem with bagging is that it’s a greedy algorithm like a single decision tree hence it tries to minimize error without looking for the optimal split. Due to this greedy approach, it fails to split data in a way that results in generating sub-trees which predicts uncorrelated results. When splitting a node during the construction of a tree, the split that is chosen is not best among all features. Instead split which is picked will be best on a random subset of features. It does not choose split which is best among all features.\n",
        "\n",
        "Random Forests changes algorithm in a way that when doing split it looks for all possible split and chooses optimal split which generates sub-trees that have less correlation. Random forests also average results of various sub-trees when doing prediction but it’s during training when doing an optimal split of data, it differs from Bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwgsBU4YUH0r",
        "colab_type": "text"
      },
      "source": [
        "**Extremely Randomized Trees**\n",
        "\n",
        "Scikit-Learn also provides another version of Random Forests which is further randomized in selecting split. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_QrDy9grtNT",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 RandomForestRegressor\n",
        "\n",
        "We'll be explaining the usage of RandomForestRegressor by using the Boston housing data set. We'll first train the model with default parameters and then do hyper-parameter tuning.# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X48Z7JqzGIpw",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1 -Train/Test Split Boston Dataset###\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlAjNAu2GW-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " X_train, X_test, Y_train, Y_test = train_test_split(X_boston, Y_boston, train_size=0.80, test_size=0.20, random_state=123)\n",
        "print('Train/Test Sets Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PBR08aHYFtN",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2  Fitting Model To Train Data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkN_apUzYRS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rforest_regressor = RandomForestRegressor(random_state=1)\n",
        "rforest_regressor.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TPHlG0jYYSI",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3 Evaluating Trained Model On Test Data.###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target varible on Test Set passed to it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IZSh6AYmlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = rforest_regressor.predict(X_test)\n",
        "\n",
        "print(Y_preds[:10])\n",
        "print(Y_test[:10])\n",
        "\n",
        "print('Training Coefficient of R^2 : %.3f'%rforest_regressor.score(X_train, Y_train))\n",
        "print('Test Coefficient of R^2 : %.3f'%rforest_regressor.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMoh36qYtzI",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4  Finetuning Model By Doing Grid Search On Various Hyperparameters###\n",
        "Below is a list of common hyperparameters that need tuning for getting the best fit for our data. We'll try various hyperparameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n",
        "\n",
        " - n_estimators - Number of base estimators whose results will be combined to produce final prediction. default=10\n",
        " - max_depth - It defines how finely tree can separate samples (list of \"if-else\" questions to ask deciding target variable). As we increase max_depth, model overfits and less value of max_depth results in underfit. We need to find best value. If no value is provided then by default None is used.\n",
        " - min_samples_split - Number of samples required to split internal node. It accepts int(0-n_samples), float(0.0-0.5] values. Float takes ceil(min_samples_split * n_samples) features.\n",
        " - min_samples_leaf - Minimum number of samples required to be at leaf node. It accepts int(0-n_samples), float(0.0-0.5] values. Float takes ceil(min_samples_leaf * n_samples) features.\n",
        " - criterion - Cost function which we algorithm tries to minimize. Currently it supports mse(mean squared error) & mae(mean absolute error).\n",
        " - max_features - Number of features to consider when doing split. It accepts int(0-n_features), float(0.0-0.5], string(sqrt, log2, auto) or None as value.\n",
        "   - None - n_features are used as value if None is provided.\n",
        "   - sqrt - sqrt(n_features) features are used for split.\n",
        "   - auto - sqrt(n_features) features are used for split.\n",
        "   = log2 - log2(n_features) features are used for split.\n",
        " - bootstrap - Decides whether samples are drawn with replacement. True = With Replacement. False = Without Replacement.default=True #* max_leaf_nodes -\n",
        "\n",
        "We'll below try various values for the above-mentioned hyper-parameters to find the best estimator for our dataset by doing 3-fold cross-validation on data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbyhqv9CYfBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "n_samples = X_boston.shape[0]\n",
        "n_features = X_boston.shape[1]\n",
        "\n",
        "params = {'n_estimators': [20,50,100],\n",
        "          'max_depth': [None, 2, 5],\n",
        "          'min_samples_split': [2, 0.5, n_samples//2, ],\n",
        "          'min_samples_leaf': [1, 0.5, n_samples//2, ],\n",
        "          'criterion': ['mse', 'mae'],\n",
        "          'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5, n_features//2,  ],\n",
        "          'bootstrap':[True, False]\n",
        "         }\n",
        "\n",
        "rf_regressor_grid = GridSearchCV(RandomForestRegressor(random_state=1), param_grid=params, n_jobs=-1, cv=3, verbose=1)\n",
        "rf_regressor_grid.fit(X_train,Y_train)\n",
        "\n",
        "print('Train R^2 Score : %.3f'%rf_regressor_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test R^2 Score : %.3f'%rf_regressor_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best R^2 Score Through Grid Search : %.3f'%rf_regressor_grid.best_score_)\n",
        "print('Best Parameters : ',rf_regressor_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbRXXxlLZJQB",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5  Printing First Few Cross Validation Results###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxs-NikBZYaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(rf_regressor_grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRKaQkCdZa_D",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 ExtraTreesRegressor###\n",
        "We'll be explaining the usage of ExtraTreesRegressor by using the Boston housing data set. We'll first train the model with default parameters and then do hyper-parameter tuning. We'll also be comparing the performance of tuned extra trees regression estimator with random forest, decision tree, and extra tree estimator of scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuAffqQUZtcc",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1 Fitting Model To Train Data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoywuS2eZz9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "extra_forest_regressor = ExtraTreesRegressor(random_state=1)\n",
        "extra_forest_regressor.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mxkYIRdZ_oh",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 Evaluating Trained Model On Test Data\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwTLToFjfz9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = extra_forest_regressor.predict(X_test)\n",
        "print(Y_preds[:10])\n",
        "print(Y_test[:10])\n",
        "print('Training Coefficient of R^2 : %.3f'%extra_forest_regressor.score(X_train, Y_train))\n",
        "print('Test Coefficient of R^2 : %.3f'%extra_forest_regressor.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYb8zKnyaT3j",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3 Finetuning Model By Doing Grid Search On Various Hyperparameters.###\n",
        "ExtraTreesRegressor has the same parameters to tune as that of RandomForestRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_2pFKJNaY3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "n_samples = X_boston.shape[0]\n",
        "n_features = X_boston.shape[1]\n",
        "\n",
        "params = {'n_estimators': [20,50,100],\n",
        "          'max_depth': [None, 2,5,],\n",
        "          'min_samples_split': [2, 0.5, n_samples//2, ],\n",
        "          'min_samples_leaf': [1, 0.5, n_samples//2, ],\n",
        "          'criterion': ['mse', 'mae'],\n",
        "          'max_features': [None, 'sqrt', 'auto', 'log2', 0.3, 0.5, n_features//2],\n",
        "          'bootstrap':[True, False]\n",
        "         }\n",
        "\n",
        "ef_regressor_grid = GridSearchCV(ExtraTreesRegressor(random_state=1), param_grid=params, n_jobs=-1, cv=3, verbose=1)\n",
        "ef_regressor_grid.fit(X_train,Y_train)\n",
        "\n",
        "print('Train R^2 Score : %.3f'%ef_regressor_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test R^2 Score : %.3f'%ef_regressor_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best R^2 Score Through Grid Search : %.3f'%ef_regressor_grid.best_score_)\n",
        "print('Best Parameters : ',ef_regressor_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmUKHytaeSp",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4 Printing First Few Cross Validation Results###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D4RHwLXanVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " cross_val_results = pd.DataFrame(ef_regressor_grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJzbZLezatfA",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.5  Comparing Performance Of Random Forest With Decision Tree/Extra Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iv9Oezpa7kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " rforest_regressor = ensemble.RandomForestRegressor(random_state=1)\n",
        "rforest_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(rforest_regressor.__class__.__name__,\n",
        "                                                     rforest_regressor.score(X_train, Y_train),rforest_regressor.score(X_test, Y_test)))\n",
        "\n",
        "rforest_regressor = ensemble.RandomForestRegressor(random_state=1, **rf_regressor_grid.best_params_)\n",
        "rforest_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(rforest_regressor.__class__.__name__,\n",
        "                                                     rforest_regressor.score(X_train, Y_train),rforest_regressor.score(X_test, Y_test)))\n",
        "\n",
        "\n",
        "extra_forest_regressor = ensemble.ExtraTreesRegressor(random_state=1)\n",
        "extra_forest_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_forest_regressor.__class__.__name__,\n",
        "                                                     extra_forest_regressor.score(X_train, Y_train),extra_forest_regressor.score(X_test, Y_test)))\n",
        "\n",
        "extra_forest_regressor = ensemble.ExtraTreesRegressor(random_state=1, **ef_regressor_grid.best_params_)\n",
        "extra_forest_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_forest_regressor.__class__.__name__,\n",
        "                                                     extra_forest_regressor.score(X_train, Y_train),extra_forest_regressor.score(X_test, Y_test)))\n",
        "\n",
        "dtree_regressor = tree.DecisionTreeRegressor(random_state=1)\n",
        "dtree_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(dtree_regressor.__class__.__name__,\n",
        "                                                     dtree_regressor.score(X_train, Y_train),dtree_regressor.score(X_test, Y_test)))\n",
        "\n",
        "extra_tree_regressor = tree.ExtraTreeRegressor(random_state=1)\n",
        "extra_tree_regressor.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_forest_regressor.__class__.__name__,\n",
        "                                                     extra_tree_regressor.score(X_train, Y_train),extra_tree_regressor.score(X_test, Y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc2pIQwqbBWr",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 RandomForestClassifier##\n",
        "We'll be explaining the usage of RandomForestClassifier by using digits data set. We'll first train the model with default parameters and then do hyper-parameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vta4YRKmh8tA",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1 Train/Test Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MuemGYtbMqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size=0.80, test_size=0.20, random_state=123)\n",
        "print('Train/Test Sets Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUDVzM0bSLi",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2 Fitting Model To Train Data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkefHNeVbbd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rforest_classifier = RandomForestClassifier(random_state=1)\n",
        "rforest_classifier.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVoEDoy7i2vR",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3 Evaluating Trained Model On Test Data\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCIcBUITjpan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = rforest_classifier.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "\n",
        "print('Test Accuracy : %.3f'%(Y_preds == Y_test).mean() )\n",
        "print('Test Accuracy : %.3f'%rforest_classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%rforest_classifier.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY7vkIbujzLo",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.4 Finetuning Model By Doing Grid Search On Various Hyperparameters\n",
        "RandomForestClassifier has the same parameters to tune as that of RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH9KbsjOj6O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "n_samples = X_digits.shape[0]\n",
        "n_features = X_digits.shape[1]\n",
        "\n",
        "params = {'n_estimators': [20,50,100],\n",
        "          'max_depth': [None, 2, 5,],\n",
        "          'min_samples_split': [2, 0.5, n_samples//2, ],\n",
        "          'min_samples_leaf': [1, 0.5, n_samples//2, ],\n",
        "          'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5, n_features//2, ],\n",
        "          'bootstrap':[True, False]\n",
        "         }\n",
        "\n",
        "rf_classifier_grid = GridSearchCV(RandomForestClassifier(random_state=1), param_grid=params, n_jobs=-1, cv=3, verbose=1)\n",
        "rf_classifier_grid.fit(X_train,Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%rf_classifier_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accurqacy : %.3f'%rf_classifier_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%rf_classifier_grid.best_score_)\n",
        "print('Best Parameters : ',rf_classifier_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duHTJ3RrkNk4",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.5 Printing First Few Cross Validation Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5Hpv8LlkUsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(rf_classifier_grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urMpr121kgf4",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 ExtraTreesClassifier \n",
        "We'll be explaining the usage of ExtraTreesClassifier by using digits data set. We'll first train the model with default parameters and then do hyper-parameter tuning. We'll also be comparing the performance of tuned extra trees regression estimator with random forest, decision tree, and extra tree estimator of scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVusWikkxws",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.1  Fitting Model To Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40vmRfWJk7ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "extra_forest_classifier = ensemble.ExtraTreesClassifier(random_state=1)\n",
        "extra_forest_classifier.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-1DtfblGat",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.2 Evaluating Trained Model On Test Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od8IKHJmlNed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = extra_forest_classifier.predict(X_test)\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "print('Test Accuracy : %.3f'%(Y_preds == Y_test).mean())\n",
        "print('Test Accuracy : %.3f'%extra_forest_classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%extra_forest_classifier.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM9Da208lZGH",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.3 Finetuning Model By Doing Grid Search On Various Hyperparameters\n",
        "ExtraTreesClassifier has the same parameters to tune as that of RandomForestRegressor/ExtraTreesRegressor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvrZ9FkWljXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "n_samples = X_digits.shape[0]\n",
        "n_features = X_digits.shape[1]\n",
        "\n",
        "params = {'n_estimators': [20,50,100],\n",
        "          'max_depth': [None, 2, 5,],\n",
        "          'min_samples_split': [2, 0.5, n_samples//2, ],\n",
        "          'min_samples_leaf': [1, 0.5, n_samples//2, ],\n",
        "          'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5, n_features//2, ],\n",
        "          'bootstrap':[True, False]\n",
        "         }\n",
        "\n",
        "ef_classifier_grid = GridSearchCV(ExtraTreesClassifier(random_state=1), param_grid=params, n_jobs=-1, cv=3, verbose=1)\n",
        "ef_classifier_grid.fit(X_train,Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%ef_classifier_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accurqacy : %.3f'%ef_classifier_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%ef_classifier_grid.best_score_)\n",
        "print('Best Parameters : ',ef_classifier_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPQu1zCkl4Ln",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.4 Printing First Few Cross Validation Results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIA5kB6ml9je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(ef_classifier_grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7TqgX-omRSE",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.5 Comparing Performance Of Random Forest With Decision Tree/Extra Tree "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ism1yV7mmZXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest_classifier = ensemble.RandomForestClassifier(random_state=1)\n",
        "rforest_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(rforest_classifier.__class__.__name__,\n",
        "                                                     rforest_classifier.score(X_train, Y_train),rforest_classifier.score(X_test, Y_test)))\n",
        "\n",
        "rforest_classifier = ensemble.RandomForestClassifier(random_state=1, **rf_classifier_grid.best_params_)\n",
        "rforest_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(rforest_classifier.__class__.__name__,\n",
        "                                                     rforest_classifier.score(X_train, Y_train),rforest_classifier.score(X_test, Y_test)))\n",
        "\n",
        "extra_forest_classifier = ensemble.ExtraTreesClassifier(random_state=1)\n",
        "extra_forest_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_forest_classifier.__class__.__name__,\n",
        "                                                     extra_forest_classifier.score(X_train, Y_train),extra_forest_classifier.score(X_test, Y_test)))\n",
        "\n",
        "extra_forest_classifier = ensemble.ExtraTreesClassifier(random_state=1, **ef_classifier_grid.best_params_)\n",
        "extra_forest_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_forest_classifier.__class__.__name__,\n",
        "                                                     extra_forest_classifier.score(X_train, Y_train),extra_forest_classifier.score(X_test, Y_test)))\n",
        "\n",
        "dtree_classifier = tree.DecisionTreeClassifier(random_state=1)\n",
        "dtree_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(dtree_classifier.__class__.__name__,\n",
        "                                                     dtree_classifier.score(X_train, Y_train),dtree_classifier.score(X_test, Y_test)))\n",
        "\n",
        "extra_tree_classifier = tree.ExtraTreeClassifier(random_state=1)\n",
        "extra_tree_classifier.fit(X_train, Y_train)\n",
        "print(\"%s : Train Accuracy : %.2f, Test Accuracy : %.2f\"%(extra_tree_classifier.__class__.__name__,\n",
        "                                                     extra_tree_classifier.score(X_train, Y_train),extra_tree_classifier.score(X_test, Y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "- Ensemble Learning\n",
        "https://coderzcolumn.com/tutorials/machine-learning/scikit-learn-sklearn-ensemble-learning-boosting\n",
        "- Scikit-Learn - Ensemble Learning : Bootstrap Aggregation(Bagging) & Random Forests\n",
        "https://coderzcolumn.com/tutorials/machine-learning/scikit-learn-sklearn-ensemble-learning-bagging-and-random-forests"
      ]
    }
  ]
}