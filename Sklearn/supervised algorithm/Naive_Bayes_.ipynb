{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Naive Bayes .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Machine-Learning/blob/master/Sklearn/supervised%20algorithm/Naive_Bayes_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes \n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Naive Bayes estimators are probabilistic estimators based on the Bayes theorem with assumptions that there is strong independence between features. The Bayes Theorem helps us find out the probability of occurring events based on some prior knowledge of conditions that can be related to the event. The naive Bayes classifiers have worked quite well for document classification and spam filtering applications. It requires a small amount of training data to set up with probabilities for Bayes theorem and therefore works quite fast.\n",
        "\n",
        "Scikit-Learn provides a list of 4 Naive Bayes estimators where each differs from other based on probability of particular feature appearing if particular class appears:\n",
        "   - BernoulliNB - It represents classifier which is based on data that is    multivariate Bernoulli distributions. The Bernoulli distribution implies that data can have multiple features but each one is assumed to be a binary variable.\n",
        "   - GaussianNB - It represents classifier which is based on assumption that likelihood of features is Gaussian distribution.\n",
        "   - ComplementNB - It represents a classifier that uses a complement of each class to compute model weights. It's a standard variant of multinomial naive Bayes which is well suited for imbalanced class classification problems.\n",
        "   - MultinomialNB - It represents a classifier that is suited for multinomially distributed data.\n",
        "\n",
        "We'll be explaining the usage of each one of the naive Bayes variants with examples.\n",
        "We'll start by importing the necessary libraries for our tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT8KNN_admkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Machine-Learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Importing necessary libraries ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "np.set_printoptions(precision=2)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Load Datasets ##\n",
        "We'll be using digits dataset for our explanation purpose. It has data about every 0-9 digits as an 8x8 pixel image. Each sample image is kept as a vector of size 64.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1wsGi52CAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_boston, load_digits\n",
        "digits = load_digits()\n",
        "X_digits, Y_digits = digits.data, digits.target\n",
        "print('Dataset Size : ', X_digits.shape, Y_digits.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Splitting Data Into Train/Test Sets ##\n",
        "We'll split the dataset into two parts:\n",
        "  - Training data which will be used for the training model.\n",
        "  - Test data against which accuracy of the trained model will be checked.\n",
        "\n",
        "train_test_split function of model_selection module of sklearn will help us split data into two sets with 80% for training and 20% for test purposes. We are also using seed(random_state=123) with train_test_split so that we always get the same split and can reproduce results in the future as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9JGZRKCCAcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size=0.80, test_size=0.20, stratify=Y_digits, random_state=123)\n",
        "print('Train/Test Sizes : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5gnX_D54aZ1",
        "colab_type": "text"
      },
      "source": [
        "## 4. BernoulliNB##\n",
        "The first estimator that we'll be introducing is BernoulliNB available with the naive_bayes module of sklearn. We'll be first fitting it with default parameters to data and then will try to improve its performance by doing hyperparameter tuning. We'll also evaluate its performance using a confusion matrix. We'll even inform you regarding important attributes of BernoulliNB which can give helpful insight once the model is trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "###4.1 - Fitting Default Model To Train Data###\n",
        "We'll be fitting model to train data by using fit() method of estimator passing it train features and train labels. We are fitting a default model to train data without setting any parameter explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "bernoulli_nb =  BernoulliNB()\n",
        "bernoulli_nb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "### 4.2-Evaluating Trained Model On Test Data ###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = bernoulli_nb.predict(X_test)\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "print('Test Accuracy : %.3f'%bernoulli_nb.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%bernoulli_nb.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 -Plotting Confusion Matrix###\n",
        "We'll be plotting the confusion matrix to better understand the performance of our model. We have designed the method plot_confusion_matrix() which accepts original labels and predicted labels of data. It then plots a confusion matrix. We'll be reusing this method in the future as well when training other estimators.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(Y_test, Y_preds):\n",
        "    conf_mat = confusion_matrix(Y_test, Y_preds)\n",
        "    #print(conf_mat)\n",
        "    fig = plt.figure(figsize=(6,6))\n",
        "    plt.matshow(conf_mat, cmap=plt.cm.Blues, fignum=1)\n",
        "    plt.yticks(range(10), range(10))\n",
        "    plt.xticks(range(10), range(10))\n",
        "    plt.colorbar();\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            plt.text(i-0.2,j+0.1, str(conf_mat[j, i]), color='tab:red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkXFMVZP76K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, bernoulli_nb.predict(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WNB53OY8IfG",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Important Attributes of BernoulliNB###\n",
        "Below are list of important attributes available through estimator instance of BernoulliNB.\n",
        "\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - feature_log_prob_ - It represents log probability of particular feature based on class. (n_classes x n_features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ykAt_v8egH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bernoulli_nb.class_log_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBDfWMwU8m8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Log Probability of Each Feature per class : \", bernoulli_nb.feature_log_prob_.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdpTOYOlCAdP",
        "colab_type": "text"
      },
      "source": [
        "### 4.5-Finetuning Model By Doing Grid Search On Various Hyperparameters Ridge Regression\n",
        "Below is a list of common hyperparameters that needs tuning for getting best fit for our data. We'll try various hyperparameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n",
        "\n",
        "  - alpha - It accepts float value representing the additive smoothing parameter. The value of 0.0 represents no smoothing. The default value of this parameter is 1.0.\n",
        "\n",
        "**GridSearchCV**\n",
        "\n",
        "It's a wrapper class provided by sklearn which loops through all parameters provided as params_grid parameter with a number of cross-validation folds provided as cv parameter, evaluates model performance on all combinations and stores all results in cv_results_ attribute. It also stores model which performs best in all cross-validation folds in best_estimator_ attribute and best score in best_score_ attribute.\n",
        "\n",
        "n_jobs parameter is provided by many estimators. It accepts number of cores to use for parallelization. If value of -1 is given then it uses all cores. It uses joblib parallel processing library for running things in parallel in background.\n",
        "We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uHKjQWZ0WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],\n",
        "         }\n",
        "\n",
        "bernoulli_nb_grid = GridSearchCV(BernoulliNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
        "bernoulli_nb_grid.fit(X_digits,Y_digits)\n",
        "\n",
        "print('Train Accuracy : %.3f'%bernoulli_nb_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%bernoulli_nb_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%bernoulli_nb_grid.best_score_)\n",
        "print('Best Parameters : ',bernoulli_nb_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTe0Qqi3CAeA",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 - Plotting Confusion Matrix ###\n",
        "Below we are plotting the confusion matrix again with the best estimator that we found out using grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4Ho1l8Ksjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, bernoulli_nb_grid.best_estimator_.predict(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwPfoQSS4Av",
        "colab_type": "text"
      },
      "source": [
        "## 5- GaussianNB ##\n",
        "The first estimator that we'll be introducing is GaussianNB available with the naive_bayes module of sklearn. We'll be first fitting it with default parameters to data and then will try to improve its performance by doing hyperparameter tuning. We'll also evaluate its performance using a confusion matrix. We'll even inform you regarding important attributes of GaussianNB which can give helpful insight once the model is trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZx-6XeziR0",
        "colab_type": "text"
      },
      "source": [
        "### 5.1-Fitting Default Model To Train Data¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q87nKz4zxr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gaussian_nb = GaussianNB()\n",
        "gaussian_nb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sq0XpVN0UZ7",
        "colab_type": "text"
      },
      "source": [
        "### 5.2-Evaluating Trained Model On Test Data.\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target varible on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2nyHRV0dnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = gaussian_nb.predict(X_test)\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "print('Test Accuracy : %.3f'%gaussian_nb.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%gaussian_nb.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUnd-y071Epv",
        "colab_type": "text"
      },
      "source": [
        "### 5.2- Plotting Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsBaFnzd1U_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, gaussian_nb.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Gq2A7BCAeX",
        "colab_type": "text"
      },
      "source": [
        "### 5.3 -Important Attributes of GaussianNB ###\n",
        "Below are list of important attributes available through estimator instance of GaussianNB.\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - epsilon_ - It represents absolute additive value to variances.\n",
        "  - sigma_ - It represents variance of each feature per class. (n_classes x n_features)\n",
        "  - theta_ - It represents mean of feature per class. (n_classes x n_features)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZKdf9heXoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gaussian_nb.class_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVgD1Ak6ed2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gaussian_nb.epsilon_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21fxEwfdekt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Gaussian Naive Bayes Sigma Shape : \", gaussian_nb.sigma_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS5rUYkYesAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Gaussian Naive Bayes Theta Shape : \", gaussian_nb.theta_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCpBVwViJPO",
        "colab_type": "text"
      },
      "source": [
        "## 6. ComplementNB \n",
        "The first estimator that we'll be introducing is ComplementNB available with the naive_bayes module of sklearn. We'll be first fitting it with default parameters to data and then will try to improve its performance by doing hyperparameter tuning. We'll also evaluate its performance using a confusion matrix. We'll even inform you regarding important attributes of ComplementNB which can give helpful insight once the model is trained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6as5LRNgjTng",
        "colab_type": "text"
      },
      "source": [
        "### 6.2 Fitting Model To Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld1_nNE_mDcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "complement_nb = ComplementNB()\n",
        "complement_nb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yN3QOkmDCAed",
        "colab_type": "text"
      },
      "source": [
        "### 6.3 - Evaluating Trained Model On Test Data.###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMoGy7D4KXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Y_preds = complement_nb.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "\n",
        "print('Test Accuracy : %.3f'%complement_nb.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%complement_nb.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO6HAiq04ZKr",
        "colab_type": "text"
      },
      "source": [
        "### 6.4 Plotting Confusion Matrix¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woRGjqgH4lTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, complement_nb.predict(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xH8Qq6J72il",
        "colab_type": "text"
      },
      "source": [
        "###6.5 -Important Attributes of ComplementNB¶\n",
        "\n",
        "Below are list of important attributes available through estimator instance of ComplementNB.\n",
        "\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - feature_log_prob_ - It represents log probability of particular feature      based on class. (n_classes x n_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn6-jer48jeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complement_nb.class_log_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcudcn3rgf7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Log Probability of Each Feature per class : \", complement_nb.feature_log_prob_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J75Rv0fr8qWA",
        "colab_type": "text"
      },
      "source": [
        "### 6.6 Finetuning Model By Doing Grid Search On Various Hyperparameters.\n",
        "Below is a list of common hyperparameters that needs tuning for getting best fit for our data. We'll try various hyperparameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n",
        "\n",
        "  - alpha - It accepts float value representing the additive smoothing parameter. The value of 0.0 represents no smoothing. The default value of this parameter is 1.0.\n",
        "  \n",
        "We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDoYPVl48x_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],\n",
        "         }\n",
        "\n",
        "complement_nb_grid = GridSearchCV(ComplementNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
        "complement_nb_grid.fit(X_digits,Y_digits)\n",
        "\n",
        "print('Train Accuracy : %.3f'%complement_nb_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%complement_nb_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%complement_nb_grid.best_score_)\n",
        "print('Best Parameters : ',complement_nb_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Ubl4dFR1IG",
        "colab_type": "text"
      },
      "source": [
        "###6.6 Plotting Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UlaNus5S00W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, complement_nb_grid.best_estimator_.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2MOQ4EQSOR3",
        "colab_type": "text"
      },
      "source": [
        "##7.MultinomialNB \n",
        "The first estimator that we'll be introducing is MultinomialNB available with the naive_bayes module of sklearn. We'll be first fitting it with default parameters to data and then will try to improve its performance by doing hyperparameter tuning. We'll also evaluate its performance using a confusion matrix. We'll even inform you regarding important attributes of MultinomialNB which can give helpful insight once the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH3RPskMSFz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, complement_nb_grid.best_estimator_.predict(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Bpx7DCS_Hg",
        "colab_type": "text"
      },
      "source": [
        "###7.1 Fitting Default Model To Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRZyu778bBmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinomial_nb = MultinomialNB()\n",
        "multinomial_nb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na4QkhBbbKUT",
        "colab_type": "text"
      },
      "source": [
        "###7.2 Evaluating Trained Model On Test Data\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4jiDFJnbJAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = multinomial_nb.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "\n",
        "print('Test Accuracy : %.3f'%multinomial_nb.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%multinomial_nb.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZfpXMFLbiDQ",
        "colab_type": "text"
      },
      "source": [
        "###7.3 Plotting Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXpM8bIebn7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, multinomial_nb.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkHO3kcibyVR",
        "colab_type": "text"
      },
      "source": [
        "###7.4 Important Attributes of MultinomialNB\n",
        "Below are list of important attributes available through estimator instance of MultinomialNB.\n",
        "\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - feature_log_prob_ - It represents log probability of particular feature based on class. (n_classes x n_features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJthSM_lcDZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multinomial_nb.class_log_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3NMSeXacQf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Log Probability of Each Feature per class : \", multinomial_nb.feature_log_prob_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOU8Vi0BccAJ",
        "colab_type": "text"
      },
      "source": [
        "###7.5 Finetuning Model By Doing Grid Search On Various Hyperparame\n",
        "Below is a list of common hyperparameters that needs tuning for getting best fit for our data. We'll try various hyperparameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n",
        "\n",
        "  - alpha - It accepts float value representing the additive smoothing parameter. The value of 0.0 represents no smoothing. The default value of this parameter is 1.0.\n",
        "  \n",
        "We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH7k634FcuyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],\n",
        "         }\n",
        "\n",
        "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
        "multinomial_nb_grid.fit(X_digits,Y_digits)\n",
        "\n",
        "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
        "print('Best Parameters : ',multinomial_nb_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpo2iWxdBaY",
        "colab_type": "text"
      },
      "source": [
        "###7.6 Plotting Confusion Matrix\n",
        "Below we are plotting the confusion matrix again with the best estimator that we found out using grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPRXvkRfdHcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, multinomial_nb_grid.best_estimator_.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "Scikit-Learn - Naive Bayes\n",
        "https://coderzcolumn.com/tutorials/machine-learning/scikit-learn-sklearn-naive-bayes?fbclid=IwAR2EUHN0XwJlCQ8hxjvYHh9Vl4g0AjllmD1ktHsNd7Mwu5g2bOLZEjdKld4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvqfD4BCAei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}