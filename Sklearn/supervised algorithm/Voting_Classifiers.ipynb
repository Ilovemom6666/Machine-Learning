{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voting Classifiers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL0AffZGOZxZ"
      },
      "source": [
        "# **Introduction**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEtW2gI7KVy5"
      },
      "source": [
        "Similarly, if we aggregate the predictions of a group of models (such as classifiers or regressors), we will often get better predictions than the best individual predictor. A group of predictors is called an **ensemble**. Thus this technique is called **ensemble learning**, and an ensemble learning algorithm is called an Ensemble Method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIDjC_tsK0HY"
      },
      "source": [
        "As an example of an ensemble method, we can train a **group of decision tree classifiers**, each on a random subset of the training data. **Such an ensemble of decision trees is called a random forest**. Despite its simplicity, this is one of the most powerful machine learning algorithms available today. In this chapter, we will discuss the most famous ensemble learning methods, including: **Bagging, Boosting, & Stacking.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8nF4iiAxoS"
      },
      "source": [
        "# **Voting Classifiers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJIKaFHZA7MB"
      },
      "source": [
        "Suppose we have trained a few classifiers, each achieving an 80% accuracy. A very simple way to create an even better classifiers is to aggregate the predictions of all our classifiers and choose the prediction that is the most frequent.\n",
        "\n",
        "**Majority voting classification is called Hard Voting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUPzZRuhB53n"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1Y01QJdvZ4mucKd2HIfZjnZPPdn35ISIc\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZeVqhkNGE_"
      },
      "source": [
        "Somewhat surprisingly, this classifier achieves an even better accuracy than the best predictor in the ensemble. Even if each classifier is a weak learner (does slightly better then random guessing). Assuming that we have a sufficient number of weak learners and enough diversity.\n",
        "\n",
        "Due to the law of large numbers, if we build an ensemble containing 1,000 classifiers with individual accuracies of $51%$ & trained for binary classification, If we predict the majority voting class, we can hope for up to $75%$ accuracy.\n",
        "\n",
        "This is only true if all classifiers are completely independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data.\n",
        "\n",
        "One way to get diverse classifiers is use different algorithms for each one of them & train them on different subset of the training data.\n",
        "\n",
        "Let's implement a hard voting ensemble learner using scikit-learn:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh-YhPsZCG7S"
      },
      "source": [
        "**Python implmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMfrhXQhNVob"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hprKmZLBNdNZ"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRZKUesUNiNn"
      },
      "source": [
        "log_clf = LogisticRegression(solver='lbfgs')\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "svm_clf = SVC(gamma='scale')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXyBuAIjNnBZ"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBa0B1EhNspZ"
      },
      "source": [
        "X, y = datasets.make_moons(n_samples=10000, noise=0.5)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57JwyteWNw1Q",
        "outputId": "967b0632-dda0-4799-cf94-6a17410fdea2"
      },
      "source": [
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6700, 2), (6700,), (3300, 2), (3300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUzphfgFN2V2"
      },
      "source": [
        "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)], voting='hard')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNROV_7TN6dO",
        "outputId": "fc519296-81a5-47c0-af5e-f6c8e8a599fb"
      },
      "source": [
        "voting_clf.fit(X_train, y_train)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr',\n",
              "                              LogisticRegression(C=1.0, class_weight=None,\n",
              "                                                 dual=False, fit_intercept=True,\n",
              "                                                 intercept_scaling=1,\n",
              "                                                 l1_ratio=None, max_iter=100,\n",
              "                                                 multi_class='auto',\n",
              "                                                 n_jobs=None, penalty='l2',\n",
              "                                                 random_state=None,\n",
              "                                                 solver='lbfgs', tol=0.0001,\n",
              "                                                 verbose=0, warm_start=False)),\n",
              "                             ('rf',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     cr...\n",
              "                                                     oob_score=False,\n",
              "                                                     random_state=None,\n",
              "                                                     verbose=0,\n",
              "                                                     warm_start=False)),\n",
              "                             ('svc',\n",
              "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                  class_weight=None, coef0=0.0,\n",
              "                                  decision_function_shape='ovr', degree=3,\n",
              "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                  probability=False, random_state=None,\n",
              "                                  shrinking=True, tol=0.001, verbose=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqIKJAZrOD6u"
      },
      "source": [
        "Let's take a look at the performance of each classifier + ensemble method on the validation set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I77QMfXHOHRq"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQLu9-ZtOI7n",
        "outputId": "a09fc59f-13d1-4d4c-99f1-3206e4c877d9"
      },
      "source": [
        "for clf in [log_clf, rf_clf, svm_clf, voting_clf]:\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_hat = clf.predict(X_val)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_val, y_hat))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.8151515151515152\n",
            "RandomForestClassifier 0.803939393939394\n",
            "SVC 0.8303030303030303\n",
            "VotingClassifier 0.8254545454545454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vodRbHN8OVam"
      },
      "source": [
        "There we have it! The voting classifier slightly outperforms the individual classifiers.\n",
        "\n",
        "If all ensemble method learners can estimate class probabilities, we can average their probabilities per class then predict the class with the highest probability. This is called Soft voting. It often yields results better than hard voting because it weights confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgWJOp-40ADb"
      },
      "source": [
        "# **References**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIXjbz4hOO6i"
      },
      "source": [
        "[Chapter 7. Ensemble Learning & Random Forests](https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/07.Ensembles_RFs.ipynb)"
      ]
    }
  ]
}