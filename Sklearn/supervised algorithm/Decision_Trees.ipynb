{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Decision Trees.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Decision Trees are a class of algorithms that are based on \"if\" and \"else\" conditions. Based on these conditions, decisions are made to the task at hand. These conditions are decided by an algorithm based on data at hand. How many conditions, kind of conditions, and answers to that conditions are based on data and will be different for each dataset. We'll be covering the usage of decision tree implementation available in scikit-learn for classification and regression tasks below.\n",
        "\n",
        "Below we have highlighted some characteristics of decision tree\n",
        "\n",
        "Fast to train and easy to understand & interpret.\n",
        "\n",
        "Binary splitting of questions is the essence of decision tree models.\n",
        "\n",
        "Requires little preprocessing of data.\n",
        "\n",
        "Can work with variables of different types (continuous & discrete)\n",
        "\n",
        "Invariant to feature scaling.\n",
        "\n",
        "Models are called \"nonparametric\" because there are no hyper-parameters to tune.\n",
        "\n",
        "If given more data then the model becomes more flexible\n",
        "\n",
        "We'll start by importing the necessary modules needed for our tutorial. We'll need pydotplus library installed as it'll be used to plot decision trees trained by scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiqyS4tHax4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Machine-Learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Importing necessary libraries ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pydotplus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAreMCMEIybp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "print(\"Python Version : \",sys.version)\n",
        "print(\"Scikit-Learn Version : \",sklearn.__version__)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") ## We'll silent future warnings using this command.\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "## Beow magic function fits plot inside of current notebook. \n",
        "## There is another option to it (%matplotlib notebook) which opens plot in new notebook.\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN",
        "colab_type": "text"
      },
      "source": [
        "**DecisionTreeClassifier**\n",
        "\n",
        "Below we are loading classic IRIS classification dataset provided by scikit-learn which has 150 samples of 3 categories of flowers containing 50 samples for each category (iris-setosa, iris-virginica, iris-versicolor). We'll use DecisionTreeClassifier provided by scikit-learn for the classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Loading Data ##\n",
        "Below we are loading the IRIS dataset which comes as default with the sklearn package. it returns Bunch object which is almost the same as the dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1wsGi52CAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, Y = iris.data, iris.target\n",
        "\n",
        "print('Dataset features names : '+str(iris.feature_names))\n",
        "print('Dataset features size : '+str(iris.data.shape))\n",
        "print('Dataset target names : '+str(iris.target_names))\n",
        "print('Dataset target size : '+str(iris.target.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Splitting Dataset into Train & Test sets ##\n",
        "We'll split the dataset into two parts:\n",
        "\n",
        "    Training data which will be used for the training model.\n",
        "\n",
        "    Test data against which accuracy of the trained model will be checked.\n",
        "    \n",
        "train_test_split function of the model_selection module of sklearn will help us split data into two sets with 80% for training and 20% for test purposes. We are also using seed(random_state=123) with train_test_split so that we always get the same split and can reproduce results in the future as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9JGZRKCCAcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.75, test_size=0.25, stratify=Y, random_state=123)\n",
        "print('Train/Test Sizes : ',X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "##4- Fitting Model To Train Data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree_classifier = DecisionTreeClassifier(random_state=1)\n",
        "tree_classifier.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "## 5-Evaluating Trained Model On Test Data\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it. We'll use score() which returns the accuracy of the model to check model accuracy on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = tree_classifier.predict(X_test)\n",
        "print(Y_preds)\n",
        "print(Y_test)\n",
        "print('Test Accuracy : %.3f'%(Y_preds == Y_test).mean() )\n",
        "print('Test Accuracy : %.3f'%tree_classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%tree_classifier.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMtYfEjHeRdc",
        "colab_type": "text"
      },
      "source": [
        "DecisionTreeClassifier instance provides predict_proba() method which returns probability returned by model for each class. We'll try to print probabilities predicted by the model for the first few test samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9x6QqcxeZWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree_classifier.predict_proba(X_test)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "## 6 - Finetuning Model By Doing Grid Search On Various Hyperparameters##\n",
        "\n",
        "Below is a list of common hyper-parameters that needs tuning for getting best fit for our data. We'll try various hyper-parameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy\n",
        "- criterion: It accepts string argument specifying which function to use to measure the quality of a split.\n",
        "    - gini - Gini Impurity. This is the default value.\n",
        "    - entropy - Information Gain.\n",
        "- max_depth - It defines how finely tree can separate samples (list of \"if-else\" questions to ask deciding target variable). As we increase max_depth, model over-fits, and less value of max_depth results in under-fit. We need to find the best value. If no value is provided then by default None is used.\n",
        "- max_features - Number of features to consider when doing split. It accepts int(0-n_features), float(0.0-0.5], string(sqrt, log2, auto) or None as value.\n",
        "    - None - n_features are used as value if None is provided.\n",
        "    - sqrt - sqrt(n_features) features are used for split.\n",
        "    - auto - sqrt(n_features) features are used for split.\n",
        "    - log2 - log2(n_features) features are used for split.\n",
        "- min_samples_split - Number of samples required to split internal node. It accepts int(0-n_samples), float(0.0-0.5] values. Float takes ceil(min_samples_split * n_samples) features.\n",
        "- min_samples_leaf - Minimum number of samples required to be at leaf node. It accepts int(0-n_samples), float(0.0-0.5] values. Float takes ceil(min_samples_leaf * n_samples) features.\n",
        "\n",
        "**GridSearchCV**\n",
        "\n",
        "It's a wrapper class provided by sklearn which loops through all parameters provided as params_grid parameter with a number of cross-validation folds provided as cv parameter, evaluates model performance on all combinations and stores all results in cv_results_ attribute. It also stores model which performs best in all cross-validation folds in best_estimator_ attribute and best score in best_score_ attribute.\n",
        "We'll below try various values for the above-mentioned hyper-parameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "n_features = X.shape[1]\n",
        "n_samples = X.shape[0]\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=1), cv=3, n_jobs=-1, verbose=5,\n",
        "                    param_grid ={\n",
        "                    'criterion': ['gini', 'entropy'],\n",
        "                    'max_depth': [None,1,2,3,4,5,6,7],\n",
        "                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features//2, n_features//3, ],\n",
        "                    'min_samples_split': [2,0.3,0.5, n_samples//2, n_samples//3, n_samples//5],\n",
        "                    'min_samples_leaf':[1, 0.3,0.5, n_samples//2, n_samples//3, n_samples//5]},\n",
        "                    )\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g78HCuLVCAdG",
        "colab_type": "text"
      },
      "source": [
        "## 7 - Printing First Few Cross-Validation Results ## \n",
        "GridSearchCV maintains results for all parameter combinations tried with all cross-validation splits. We can access results for all iterations as a dictionary by calling cv_results_ attribute on it. We are converting it to pandas dataframe for better visuals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTAYjq6JOpj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg-zk8uciMg_",
        "colab_type": "text"
      },
      "source": [
        "##8- Plotting Feature Importance # \n",
        "We can access the feature importance of each feature in the decision tree through feature_importances_ attributes. We have plotted it as well for better understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQLu4j6Iib4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(('seaborn', 'ggplot')):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.imshow(grid.best_estimator_.feature_importances_.reshape(1,-1), cmap=plt.cm.Blues, interpolation='nearest')\n",
        "    plt.xticks(range(4), iris.feature_names)\n",
        "    plt.yticks([])\n",
        "    plt.grid(None)\n",
        "    plt.colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GIu_3jPi0Ed",
        "colab_type": "text"
      },
      "source": [
        "## 9- Visualizing Decision Tree Using GraphViz & PyDotPl##\n",
        "We can visualize the decision tree by using graphviz. Scikit-learn provides export_graphviz() function which can let us convert tree trained to graphviz format. We can then generate a graph from it using the pydotplus library using its method graph_from_dot_data.\n",
        "\n",
        "We can easily ask questions about flower type based on flower features and get an answer from the decision tree based on True or False answer to the question. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mahzB268jLgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals.six import StringIO\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "\n",
        "export_graphviz(grid.best_estimator_, out_file=dot_data,\n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,\n",
        "                class_names=iris.target_names,\n",
        "                feature_names=iris.feature_names)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "\n",
        "Image(graph.create_png())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT-VLB8xjfuj",
        "colab_type": "text"
      },
      "source": [
        "##10- ExtraTreeClassifier##\n",
        "ExtraTreeClassifier is commonly referred to as an extremely randomized decision tree. When deciding to split samples into 2 groups based on a feature, random splits are drawn for each of randomly selected features and the best of them is selected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wexnQdb59tZT",
        "colab_type": "text"
      },
      "source": [
        "###10.1 Fitting Model To Train Data#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luhIk0B-90rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import ExtraTreeClassifier\n",
        "extra_tree_classifier = ExtraTreeClassifier(random_state=1)\n",
        "extra_tree_classifier.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRgocQiW9-dO",
        "colab_type": "text"
      },
      "source": [
        "###10.2 Evaluating Trained Model On Test Data ###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zQ035HR-PEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = extra_tree_classifier.predict(X_test)\n",
        "\n",
        "print(Y_preds)\n",
        "print(Y_test)\n",
        "\n",
        "print('Test Accuracy : %.3f'%(Y_preds == Y_test).mean() )\n",
        "print('Test Accuracy : %.3f'%extra_tree_classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%extra_tree_classifier.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_O2laHO-X_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extra_tree_classifier.predict_proba(X_test)[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIphSupp-dJ2",
        "colab_type": "text"
      },
      "source": [
        "###10.3- Finetuning Model By Doing Grid Search On Various Hyperparameters###\n",
        "ExtraTreeClassifier has same hyperparameters as that of DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg4jUe2v-qYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "5f775099-b6ab-4ab2-e3c0-95a380216f06"
      },
      "source": [
        "n_features = X.shape[1]\n",
        "n_samples = X.shape[0]\n",
        "\n",
        "grid = GridSearchCV(ExtraTreeClassifier(random_state=1), cv=3, n_jobs=-1, verbose=5,\n",
        "                    param_grid ={\n",
        "                    'criterion': ['gini', 'entropy'],\n",
        "                    'max_depth': [None,1,2,3,4,5,6,7],\n",
        "                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features//2, n_features//3, ],\n",
        "                    'min_samples_split': [2,0.3,0.5, n_samples//2, n_samples//3, n_samples//5],\n",
        "                    'min_samples_leaf':[1, 0.3,0.5, n_samples//2, n_samples//3, n_samples//5]},\n",
        "                    )\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5184 candidates, totalling 15552 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done 2312 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=-1)]: Done 8072 tasks      | elapsed:   11.8s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.982\n",
            "Test Accuracy : 0.974\n",
            "Best Score Through Grid Search : 0.955\n",
            "Best Parameters :  {'criterion': 'gini', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 15454 tasks      | elapsed:   21.6s\n",
            "[Parallel(n_jobs=-1)]: Done 15552 out of 15552 | elapsed:   21.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvO7qP_V-4oo",
        "colab_type": "text"
      },
      "source": [
        "### 10.4-Printing First Few Cross Validation Results###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRbBqF42_IYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgLxzj33_PvP",
        "colab_type": "text"
      },
      "source": [
        "###10.5- Plotting Feature Importance#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHmBk15i_YON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Feature Importance : %s\"%str(grid.best_estimator_.feature_importances_))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgvLJ3YO_dYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(('seaborn', 'ggplot')):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.imshow(grid.best_estimator_.feature_importances_.reshape(1,-1), cmap=plt.cm.Blues, interpolation='nearest')\n",
        "    plt.xticks(range(4), iris.feature_names)\n",
        "    plt.yticks([])\n",
        "    plt.grid(None)\n",
        "    plt.colorbar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trbO8CCt_oMQ",
        "colab_type": "text"
      },
      "source": [
        "##11 DecisionTreeRegressor##\n",
        "We'll now try loading the Boston dataset provided by sklearn and will try DecisionTreeRegressor on it as well with different depth of the decision tree. We'll also visualize results letter comparing performance on train and test sets with different tree depths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFqhln8sFzJ9",
        "colab_type": "text"
      },
      "source": [
        "###11.1-Loading Data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCd5m5HpF5mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston = datasets.load_boston()\n",
        "X, Y  = boston.data, boston.target\n",
        "print('Dataset features names : '+str(boston.feature_names))\n",
        "print('Dataset features size : '+str(boston.data.shape))\n",
        "print('Dataset target size : '+str(boston.target.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X48Z7JqzGIpw",
        "colab_type": "text"
      },
      "source": [
        "###11.2-Splitting Dataset into Train & Test sets###\n",
        "Below we are splitting the Boston dataset into the train set(80%) and test set(20%). We are also using seed(random_state=123) so that we always get the same split and can reproduce results in the future as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlAjNAu2GW-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a48762e-f364-4ee1-8432-70731fb48785"
      },
      "source": [
        "X_train, X_test,Y_train, Y_test = train_test_split(X, Y, train_size=0.75, test_size=0.25, random_state=1)\n",
        "print('Train/Test Set Sizes : ', X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train/Test Set Sizes :  (379, 13) (379,) (127, 13) (127,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PBR08aHYFtN",
        "colab_type": "text"
      },
      "source": [
        "###11.3 Fitting Model To Train Data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkN_apUzYRS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_regressor = DecisionTreeRegressor(random_state=1)\n",
        "tree_regressor.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TPHlG0jYYSI",
        "colab_type": "text"
      },
      "source": [
        "###11.4 Evaluating Trained Model On Test Data.###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IZSh6AYmlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = tree_regressor.predict(X_test)\n",
        "\n",
        "print(Y_preds[:10])\n",
        "print(Y_test[:10])\n",
        "\n",
        "print('Training Coefficient of R^2 : %.3f'%tree_regressor.score(X_train, Y_train))\n",
        "print('Test Coefficient of R^2 : %.3f'%tree_regressor.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDMoh36qYtzI",
        "colab_type": "text"
      },
      "source": [
        "###11.4 Finetuning Model By Doing Grid Search On Various Hyperparameters###\n",
        "DecisionTreeRegressor has same hyperparameters as DecisionTreeClassifier. We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbyhqv9CYfBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "20a91d4a-fb3d-4882-a9df-7d70ccd396a5"
      },
      "source": [
        "n_features = X.shape[1]\n",
        "n_samples = X.shape[0]\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeRegressor(random_state=1), cv=3, n_jobs=-1, verbose=5,\n",
        "                    param_grid ={\n",
        "                    'max_depth': [None,1,2,3,4,5,6,7],\n",
        "                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features//2, n_features//3, ],\n",
        "                    'min_samples_split': [2,0.3,0.5, n_samples//2, n_samples//3, n_samples//5],\n",
        "                    'min_samples_leaf':[1, 0.3,0.5, n_samples//2, n_samples//3, n_samples//5]},\n",
        "                    )\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "print('Train R^2 Score : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test R^2 Score : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best R^2 Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2592 candidates, totalling 7776 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done 2312 tasks      | elapsed:    4.9s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train R^2 Score : 0.909\n",
            "Test R^2 Score : 0.768\n",
            "Best R^2 Score Through Grid Search : 0.780\n",
            "Best Parameters :  {'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 7712 tasks      | elapsed:   13.2s\n",
            "[Parallel(n_jobs=-1)]: Done 7776 out of 7776 | elapsed:   13.3s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbRXXxlLZJQB",
        "colab_type": "text"
      },
      "source": [
        "###11.5 Printing First Few Cross Validation Results###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxs-NikBZYaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRKaQkCdZa_D",
        "colab_type": "text"
      },
      "source": [
        "###11.6 Plotting Feature Importance###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d47L2dBdZjL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(('seaborn', 'ggplot')):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.imshow(grid.best_estimator_.feature_importances_.reshape(1,-1), cmap=plt.cm.Blues, interpolation='nearest')\n",
        "    plt.xticks(range(13), boston.feature_names)\n",
        "    plt.yticks([])\n",
        "    plt.grid(None)\n",
        "    plt.colorbar();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuAffqQUZtcc",
        "colab_type": "text"
      },
      "source": [
        "###11.6 Visualizing Decision Tree Using GraphViz & PyDotPlus###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoywuS2eZz9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dot_data = StringIO()\n",
        "export_graphviz(grid.best_estimator_, out_file=dot_data,\n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,\n",
        "                feature_names=boston.feature_names,)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "Image(graph.create_png())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mxkYIRdZ_oh",
        "colab_type": "text"
      },
      "source": [
        "##12 ExtraTreeRegressor##\n",
        "ExtraTreeRegressor like ExtraTreeClassifier is an extremely randomized decision tree for regression problems. We'll follow the same process as previous examples to explain its usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYb8zKnyaT3j",
        "colab_type": "text"
      },
      "source": [
        "###12.1 Fitting Model To Train Data###\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_2pFKJNaY3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import ExtraTreeRegressor\n",
        "\n",
        "extra_tree_regressor = ExtraTreeRegressor(random_state=1)\n",
        "extra_tree_regressor.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmUKHytaeSp",
        "colab_type": "text"
      },
      "source": [
        "###12.2 Evaluating Trained Model On Test Data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D4RHwLXanVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = extra_tree_regressor.predict(X_test)\n",
        "\n",
        "print(Y_preds[:10])\n",
        "print(Y_test[:10])\n",
        "\n",
        "print('Training Coefficient of R^2 : %.3f'%extra_tree_regressor.score(X_train, Y_train))\n",
        "print('Test Coefficient of R^2 : %.3f'%extra_tree_regressor.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJzbZLezatfA",
        "colab_type": "text"
      },
      "source": [
        "###12.3Finetuning Model By Doing Grid Search On Various Hyperparameters.###\n",
        "ExtraTreeRegressor has same hyperparameters as ExtraTreeClassifier. We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iv9Oezpa7kD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "516486b1-bf6b-431f-f80f-d9dc5d652f56"
      },
      "source": [
        "n_features = X.shape[1]\n",
        "n_samples = X.shape[0]\n",
        "\n",
        "grid = GridSearchCV(ExtraTreeRegressor(random_state=1), cv=3, n_jobs=-1, verbose=5,\n",
        "                    param_grid ={\n",
        "                    'max_depth': [None,1,2,3,4,5,6,7],\n",
        "                    'max_features': [None, 'sqrt', 'auto', 'log2', 0.3,0.5,0.7, n_features//2, n_features//3, ],\n",
        "                    'min_samples_split': [2,0.3,0.5, n_samples//2, n_samples//3, n_samples//5],\n",
        "                    'min_samples_leaf':[1, 0.3,0.5, n_samples//2, n_samples//3, n_samples//5]},\n",
        "                    )\n",
        "\n",
        "grid.fit(X_train, Y_train)\n",
        "print('Train R^2 Score : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test R^2 Score : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best R^2 Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 2592 candidates, totalling 7776 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done 4358 tasks      | elapsed:    7.8s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train R^2 Score : 0.907\n",
            "Test R^2 Score : 0.780\n",
            "Best R^2 Score Through Grid Search : 0.707\n",
            "Best Parameters :  {'max_depth': 7, 'max_features': 0.7, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 7776 out of 7776 | elapsed:   12.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc2pIQwqbBWr",
        "colab_type": "text"
      },
      "source": [
        "### 12.4 Printing First Few Cross Validation Results###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MuemGYtbMqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUDVzM0bSLi",
        "colab_type": "text"
      },
      "source": [
        "###12.5 Plotting Feature Importance###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av2EJwXxbX03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Feature Importance : %s\"%str(grid.best_estimator_.feature_importances_))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkefHNeVbbd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(('seaborn', 'ggplot')):\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.imshow(grid.best_estimator_.feature_importances_.reshape(1,-1), cmap=plt.cm.Blues, interpolation='nearest')\n",
        "    plt.xticks(range(13), boston.feature_names)\n",
        "    plt.yticks([])\n",
        "    plt.grid(None)\n",
        "    plt.colorbar();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "- Decision Trees\n",
        "https://coderzcolumn.com/tutorials/machine-learning/scikit-learn-sklearn-decision-trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvqfD4BCAei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}