{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction of SKLEARN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Machine-Learning/blob/master/Sklearn/Introduction_of_SKLEARN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lXFVQaIPBks"
      },
      "source": [
        "# **1- What is Scikit Learn**\n",
        "\n",
        "is most popular libary of machine leanring \n",
        "Scikit learn is a library used to perform machine learning in Python. Scikit learn is an open source library which is licensed under BSD and is reusable in various contexts, encouraging academic and commercial use. It provides a range of supervised and unsupervised learning algorithms in Python. Scikit learn consists of popular algorithms and libraries. Apart from that, it also contains the following packages[2]:\n",
        "\n",
        "NumPy\n",
        "Matplotlib\n",
        "SciPy (Scientific Python)\n",
        "Scikit learn is one of the attraction where we can implement machine learning using Python. It is a free machine learning library which contains simple and efficient tools for data analysis and mining purposes[1].\n",
        "\n",
        "Scikit learn comes with sample datasets, such as iris and digits. You can import the datasets and play around with them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leMVkc3Eg9rz"
      },
      "source": [
        "# **2- Prepare Data for Sklearn**\n",
        "\n",
        "**Preparing data for scikit-learn**\n",
        "- scikit-learn expects a particular structure of data:\n",
        "(samples, features)\n",
        "- Make sure that your data is atleasttwo-dimensiona\n",
        "- Make sure the rst dimension is samples\n",
        "\n",
        "**If your data is not shaped properly**\n",
        "\n",
        "Ifthe axes are swapped:\n",
        "- array.T.shape\n",
        "\n",
        "**If your data is not shaped properly**\n",
        "\n",
        "If we're missing an axis, use .reshape() :\n",
        "- array.shape\n",
        "- (10,)\n",
        "- rray.reshape([-1, 1]).shape\n",
        "- (10, 1)\n",
        "- -1 will automatically llthat axis with remaining values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EEdY_ieugBw"
      },
      "source": [
        "# **3- Supervised Learning**[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Nuw1B7utjE"
      },
      "source": [
        "## **3.1-Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbM5xxSau2lE"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UCNyTtIu6zt"
      },
      "source": [
        "# step-1: creating model class object \n",
        "model = LogisticRegression()\n",
        "# step-2: fitting training data\n",
        "model.fit(X_train, y_train)\n",
        "# step-3: using model to predict target class\n",
        "model.predict(X_test)\n",
        "# optional: using model to predict probability for each target class\n",
        "model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FvifEogvQE2"
      },
      "source": [
        "## **3.2K-Nearest Neighbors (KNN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tet0DYR4vYf9"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# step-1: creating model class object \n",
        "model = KNeighborsClassifier(n = 5) \n",
        "# step-2: fitting training data\n",
        "model.fit(X_train, y_train)\n",
        "# step-3: using model to predict target class\n",
        "model.predict(X_test)\n",
        "# optional: using model to predict probability for each target class\n",
        "model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttr-4JYf59dx"
      },
      "source": [
        "## **3.3 Decision Trees Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3PAu21Q6FlL"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# step-1: creating model class object \n",
        "model = DecisionTreeClassifier(max_depth = 10) \n",
        "# step-2: fitting training data\n",
        "model.fit(X_train, y_train)\n",
        "# step-3: using model to predict target class\n",
        "model.predict(X_test)\n",
        "# optional: using model to predict probability for each target class\n",
        "model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSPBahwA77QJ"
      },
      "source": [
        "## **3.4 Random Forest Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvsvi3H8IGw"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# step-1: creating model class object \n",
        "model = RandomForestClassifier(n_estimators = 50) \n",
        "# step-2: fitting training data\n",
        "model.fit(X_train, y_train)\n",
        "# step-3: using model to predict target class\n",
        "model.predict(X_test)\n",
        "# optional: using model to predict probability for each target class\n",
        "model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2akIh6Pi8LXp"
      },
      "source": [
        "## **3.5 Support Vector Machines (SVM)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HWCiLyG8YSw"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# step-1: creating model class object \n",
        "model = SVC() \n",
        "# step-2: fitting training data\n",
        "model.fit(X_train, y_train)\n",
        "# step-3: using model to predict target class\n",
        "model.predict(X_test)\n",
        "# optional: using model to predict probability for each target class\n",
        "model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8es9nRYg8hU6"
      },
      "source": [
        "## **3.6 Na√Øve Bayes Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APxMf30J8oPQ"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# step-1: creating model class object \n",
        "model = GaussianNB() \n",
        "# step-2: fitting training data\n",
        "model.fit(X_train, y_train)\n",
        "# step-3: using model to predict target class\n",
        "model.predict(X_test)\n",
        "# optional: using model to predict probability for each target class\n",
        "model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0t6E5SpU2E6"
      },
      "source": [
        "# **4 Supervised-learning with scikit-learn** [3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkNZ2yYisY4l"
      },
      "source": [
        "#**1- Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rGApnBJVQM9"
      },
      "source": [
        "## **1.2 Supervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emvYvPAgpNVq"
      },
      "source": [
        "##**1.2 Exploratory data analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jH5AyYYCto7"
      },
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "iris=datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdjbW9ycC1u3"
      },
      "source": [
        "type(iris)\n",
        "#output sklearn.dataset.base.bunch (similar to dictonary that contain keyvalue pairs )\n",
        "print(iris.keys())\n",
        "# Print the keys, we see that they are the feature name\n",
        "type(iris.data), type(iris.target)\n",
        "#(numpy.ndaray, numpy.ndarray)\n",
        "iris.data.shape\n",
        "# tell us that there 150 row and 4 column\n",
        "iris.target_names \n",
        "# arrat['setosa','versocolor','virginica'],dtype=]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-6XHQmSmy_o",
        "outputId": "3b2358a4-810c-4a95-dfcc-b0270749902c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "In [12]: X = iris.data\n",
        "In [13]: y = iris.target\n",
        "#Build a DataFram of the feature data and also passing column name\n",
        "In [14]: df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "In [15]: print(df.head())\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQBHDBm8nivZ"
      },
      "source": [
        "**Visual EDA**\n",
        "\n",
        "Use the pandas function scatter matrix to visuliz our dataset. we pass it the our DataFram , along with our target variable as argument to the parameter c\n",
        "which stands for color ensuring that our data points in our figure will be colored by their species. we also pass a list to fig zie which specifies the size of our figure , as well as marker size and shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgR77wmeouJn"
      },
      "source": [
        "In [16]: 1_ = pd.scatter_matrix(df, c = y, figsize = [8, 8],\n",
        " ...: s=150, marker = 'D') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5z5bOuEqM9l"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1pYaLnwUmAZhoTOLIVJqg7NgfLkdNuBhT)\n",
        "\n",
        "The result is a matrix of figure, which on the diagonal are histograms of the features correspondng to the row and column. The off-Diagonal figures are scatter plots of the column features versus row features colored by the target variable. See here for example that petal and width and length are high correlated , as you may expected and that flowers are clustered according to species. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0QtEjtZ8vN5"
      },
      "source": [
        "###**1.2.1 Numerical EDA**\n",
        "In this chapter, you'll be working with a dataset obtained from the [Machine Learning Repository ](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records) consisting of votes made by US House of Representatives Congressmen. Your goal will be to predict their party affiliation ('Democrat' or 'Republican') based on how they voted on certain key issues. Here, it's worth noting that we have preprocessed this dataset to deal with missing values. This is so that your focus can be directed towards understanding how to train and evaluate supervised learning models. Once you have mastered these fundamentals, you will be introduced to preprocessing techniques in Chapter 4 and have the chance to apply them there yourself - including on this very same dataset!\n",
        "\n",
        "Before thinking about what supervised learning models you can apply to this, however, you need to perform Exploratory data analysis (EDA) in order to understand the structure of the data. For a refresher on the importance of EDA, check out the first two chapters of Statistical Thinking in Python (Part 1).\n",
        "\n",
        "Get started with your EDA now by exploring this voting records dataset numerically. It has been pre-loaded for you into a DataFrame called df. Use pandas' .head(), .info(), and .describe() methods in the IPython Shell to explore the DataFrame, and select the statement below that is not true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nJsQvvEEyrL"
      },
      "source": [
        "- he DataFrame has a total of 435 rows and 17 columns.\n",
        "- Except for 'party', all of the columns are of type int64.\n",
        "- The first two rows of the DataFrame consist of votes made by Republicans and the next three rows consist of votes made by Democrats.\n",
        "- **There are 17 predictor variables, or features, in this DataFrame**.\n",
        "- The target variable in this DataFrame is 'party'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eaP-kp1FGso"
      },
      "source": [
        "### **1.2.2 Visual EDA**\n",
        "\n",
        "The Numerical EDA you did in the previous exercise gave you some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. Following this with some visual EDA will give you an even better understanding of the data. In the video, Hugo used the scatter_matrix() function on the Iris data for this purpose. However, you may have noticed in the previous exercise that all the features in this dataset are binary; that is, they are either 0 or 1. So a different type of plot would be more useful here, such as Seaborn's countplot.\n",
        "\n",
        "Given on the right is a countplot of the 'education' bill, generated from the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMAvUrieJzn3"
      },
      "source": [
        "plt.figure()\n",
        "sns.countplot(x='education', hue='party', data=df, palette='RdBu')\n",
        "plt.xticks([0,1], ['No', 'Yes'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzTIrJ0lJ8iT"
      },
      "source": [
        "In sns.countplot(), we specify the x-axis data to be 'education', and hue to be 'party'. Recall that 'party' is also our target variable. So the resulting plot shows the difference in voting behavior between the two parties for the 'education' bill, with each party colored differently. We manually specified the color to be 'RdBu', as the Republican party has been traditionally associated with red, and the Democratic party with blue.\n",
        "\n",
        "It seems like Democrats voted resoundingly against this bill, compared to Republicans. This is the kind of information that our machine learning model will seek to learn when we try to predict party affiliation solely based on voting behavior. An expert in U.S politics may be able to predict this without machine learning, but probably not instantaneously - and certainly not if we are dealing with hundreds of samples!\n",
        "\n",
        "In the IPython Shell, explore the voting behavior further by generating countplots for the 'satellite' and 'missile' bills, and answer the following question: Of these two bills, for which ones do Democrats vote resoundingly in favor of, compared to Republicans? Be sure to begin your plotting statements for each figure with plt.figure() so that a new figure will be set up. Otherwise, your plots will be overlayed onto the same figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s67cZPyvKJX0"
      },
      "source": [
        "- 'satellite'.\n",
        "- 'missile'.\n",
        "-**Both 'satellite' and 'missile'**\n",
        "- Neither 'satellite' nor 'missile'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq1KfgzwsEjY"
      },
      "source": [
        "##**1.3-The classification challenge**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQadfJ15wIJN"
      },
      "source": [
        "**k-Nearest Neighbors**\n",
        "\n",
        "Basic idea: Predict the label of a data point by\n",
        "\n",
        "‚óè Looking at the ‚Äòk‚Äô closest labeled data points\n",
        "\n",
        "‚óè Taking a majority vote"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcD5dpPBx3tz"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ED7iMIVf1QNHyLDwzZ4lL6q-6oyHroPh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehjaX7Zmy08-"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1mMD4oOnIaX1ZC_6zgL4Ei3sPnk3_e2i5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRYCxmmk2hRQ"
      },
      "source": [
        "**Scikit-learn fit and predict**\n",
        "\n",
        "All machine learning models implemented as Python classes\n",
        "- They implement the algorithms for learning and predicting\n",
        "- Store the information learned from the data\n",
        "\n",
        "Training a model on the data = ‚Äòfi\"ing‚Äô a model to the data\n",
        "- .fit() method\n",
        "- To predict the labels of new data: .predict() method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3edMLOVk3C1i"
      },
      "source": [
        "**Using scikit-learn to fit a classifier**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8zAiew_3O94"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "knn.fit(iris['data'], iris['target'])\n",
        "KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
        "metric='minkowski',metric_params=None, n_jobs=1,\n",
        "n_neighbors=6, p=2,weights='uniform')\n",
        "iris['data'].shape\n",
        "(150, 4)\n",
        "iris['target'].shape\n",
        "(150,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l74FgqMA4K2I"
      },
      "source": [
        "**Predicting on unlabeled data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wdKA7v74Sja"
      },
      "source": [
        "prediction = knn.predict(X_new)\n",
        "X_new.shape\n",
        "(3, 4)\n",
        "print('Prediction {}‚Äô.format(prediction))\n",
        " [1 1 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9-1DFqX47CO"
      },
      "source": [
        "###**1.3.1-k-Nearest Neighbors: Fit**\n",
        "\n",
        "Having explored the Congressional voting records dataset, it is time now to build your first classifier. In this exercise, you will fit a k-Nearest Neighbors classifier to the voting dataset, which has once again been pre-loaded for you into a DataFrame df.\n",
        "\n",
        "In the video, Hugo discussed the importance of ensuring your data adheres to the format required by the scikit-learn API. The features need to be in an array where each column is a feature and each row a different observation or data point - in this case, a Congressman's voting record. The target needs to be a single column with the same number of observations as the feature data. We have done this for you in this exercise. Notice we named the feature array X and response variable y: This is in accordance with the common scikit-learn practice.\n",
        "\n",
        "Your job is to create an instance of a k-NN classifier with 6 neighbors (by specifying the n_neighbors parameter) and then fit it to the data. The data has been pre-loaded into a DataFrame called df.\n",
        "\n",
        "**instructions**\n",
        "\n",
        "Import KNeighborsClassifier from sklearn.neighbors.\n",
        "Create arrays X and y for the features and the target variable. Here this has been done for you. Note the use of .drop() to drop the target variable 'party' from the feature array X as well as the use of the .values attribute to ensure X and y are NumPy arrays. Without using .values, X and y are a DataFrame and Series respectively; the scikit-learn API will accept them in this form also as long as they are of the right shape.\n",
        "Instantiate a KNeighborsClassifier called knn with 6 neighbors by specifying the n_neighbors parameter.\n",
        "Fit the classifier to the data using the .fit() method.\n",
        "\n",
        "**Hint**\n",
        "\n",
        "To import thing from pkg, use the command from pkg import thing.\n",
        "To instantiate a KNeighborsClassifier with 6 neighbors, use the function KNeighborsClassifier() and specify the keyword argument n_neighbors=6.\n",
        "Use the .fit() method on knn with X and y passed in as the arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHt-75Yq7zwP"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "knn.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNPzK4Fs7yS3"
      },
      "source": [
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
        "           weights='uniform')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSQBFRU977Sm"
      },
      "source": [
        "###**1.3.2-k-Nearest Neighbors: Predict**\n",
        "\n",
        "Having fit a k-NN classifier, you can now use it to predict the label of a new data point. However, there is no unlabeled data available since all of it was used to fit the model! You can still use the .predict() method on the X that was used to fit the model, but it is not a good indicator of the model's ability to generalize to new, unseen data.\n",
        "\n",
        "In the next video, Hugo will discuss a solution to this problem. For now, a random unlabeled data point has been generated and is available to you as X_new. You will use your classifier to predict the label for this new data point, as well as on the training data X that the model has already seen. Using .predict() on X_new will generate 1 prediction, while using it on X will generate 435 predictions: 1 for each sample.\n",
        "\n",
        "The DataFrame has been pre-loaded as df. This time, you will create the feature array X and target variable array y yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REHdETlw8feu"
      },
      "source": [
        "**Instruction**\n",
        "\n",
        "Create arrays for the features and the target variable from df. As a reminder, the target variable is 'party'.\n",
        "Instantiate a KNeighborsClassifier with 6 neighbors.\n",
        "Fit the classifier to the data.\n",
        "Predict the labels of the training data, X.\n",
        "Predict the label of the new data point X_new."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCBPTBcN8qfQ"
      },
      "source": [
        "**Hints**\n",
        "\n",
        "To create the target variable y, select the 'party' column of df and access its .values attribute. To create the feature array X, use the .drop() method on df with 'party' and axis=1 as arguments. Then access its .values attribute.\n",
        "To instantiate the classifier, use KNeighborsClassifier and specify the number of neighbors using the n_neighbors parameter.\n",
        "Use the .fit() method with X and y as arguments to fit the classifier to the data.\n",
        "Use the .predict() method on X to predict the labels of the training data.\n",
        "Use the .predict() method on X_new to predict the label of the new data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JptFoh-pjAaQ"
      },
      "source": [
        "#**Referenes**\n",
        "[[1] Classification Algorithms Explained in 30 Minutes](https://datamahadev.com/classification-algorithms-explained-in-30-minutes/)\n",
        "\n",
        "[[2]A Beginner‚Äôs Guide To Scikit Learn](https://medium.com/edureka/scikit-learn-machine-learning-7a2d92e4dd07)\n",
        "\n",
        "[[3] supervised-learning-with-scikit-learn](https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/classification?ex=1)\n",
        "\n",
        "[[4]datacamp-machine-learning-with-scikit-learn](https://github.com/hzitoun/datacamp-machine-learning-with-scikit-learn)"
      ]
    }
  ]
}