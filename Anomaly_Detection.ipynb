{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Anomaly Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Machine-Learning/blob/master/Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# **Anomaly Detection [Outliers Detection]**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Anomaly detection is a process where you find out the list of outliers from your data. An outlier is a sample that has inconsistent data compared to other regular samples hence raises suspicion on their validity. The presence of outliers can also impact the performance of machine learning algorithms when performing supervised tasks. It can also interfere with data scaling which is a common data preprocessing step. As a part of this tutorial, we'll be discussing estimators available in scikit-learn which can help with identifying outliers from data.\n",
        "\n",
        "Below is a list of scikit-learn estimators which let us identify outliers present in data that we'll be discussing as a part of this tutorial:\n",
        "\n",
        " - KernelDensity\n",
        " - OneClassSVM\n",
        " = IsolationForest\n",
        " - LocalOutlierFactor\n",
        " \n",
        "We'll be explaining the usage of each one with various examples.\n",
        "\n",
        "Letâ€™s start by importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD1Ko0KzD67p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Machine-Learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "# **Importing necessary libraries** #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "# **Load Datasets**\n",
        "We'll start by loading two datasets that we'll be using for our explanation purpose.\n",
        "\n",
        "- Blobs Dataset - We have created a blobs dataset which has data of 3 clusters with 500 samples and 2 features per sample. We'll be using this dataset primarily for an explanation of sklearn estimators.\n",
        "- Digits Dataset - The second dataset that we'll load is digits dataset which has 1797 images of 0-9 digits. Each image is of size 8x8 which is flattened and kept as an array of size 64.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1wsGi52CAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from sklearn.datasets import make_blobs\n",
        "\n",
        "X, Y = make_blobs(n_features=2, centers=3, n_samples=500,\n",
        "                  random_state=42)\n",
        "\n",
        "print(\"Dataset Size : \", X.shape, Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CVNbUV9G1dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(\"ggplot\"):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=\"tab:green\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbk3ih0aJwq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "X_digits, Y_digits = digits.data, digits.target\n",
        "\n",
        "print(\"Dataset Size : \", X_digits.shape, Y_digits.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "# 1- **KernelDensity** \n",
        "The KernelDensity estimator is available as a part of the kde module of the neighbors module of sklearn. It helps us measure kernel density of samples which can be then used to take out outliers. It uses KDTree or BallTree algorithm for kernel density estimation.\n",
        "\n",
        "Below is a list of important parameters of KernelDensity estimator:\n",
        "\n",
        "- algorithm - It accepts string value specifying which algorithm to use for kernel density estimation. We can specify one of the below values for this parameter.\n",
        "  - auto - Default.\n",
        "  - kd_tree\n",
        "  - ball_tree\n",
        "- kernel - It accepts string which let us specify which kernel to use for estimation. We can specify one of the below values.\n",
        "  - gaussian\n",
        "  - tophat\n",
        "  - epanechnikov\n",
        "  - exponential\n",
        "  - linear\n",
        "  - cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5gnX_D54aZ1",
        "colab_type": "text"
      },
      "source": [
        "## 1.1  Fitting Model to Data\n",
        "\n",
        "We'll first fit the KernelDensity estimator to our dataset using fit() method of it and then use it for finding out outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOA7lWQ7RRTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors.kde import KernelDensity\n",
        "# Estimate density with a Gaussian kernel density estimator\n",
        "kde = KernelDensity(kernel='gaussian')\n",
        "kde.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 - Calculate Log Density Evaluations for Each Sample\n",
        "The KernelDensity estimator has a method named score_samples() which accepts dataset and returns log density evaluations for each sample of data. We'll divide these values into 95% as valid data and 5% as outliers based on the output of score_samples() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kde_X = kde.score_samples(X)\n",
        "kde_X[:5]  # contains the log-likelihood of the data. The smaller it is the rarer is the sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "## 1.3-Dividing Dataset into Valid Samples and Outliers\n",
        "Below we are trying to find out quantiles value for 5% of total data. We'll use that value to divide data into outliers and valid samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats.mstats import mquantiles\n",
        "\n",
        "alpha_set = 0.95\n",
        "tau_kde = mquantiles(kde_X, 1. - alpha_set)\n",
        "\n",
        "tau_kde"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgyAtanLYfoT",
        "colab_type": "text"
      },
      "source": [
        "All the values in kde_X array which are less than tau_kde will be outliers and values greater than it will be qualified as valid samples. We'll try to find out indexes of samples that are outliers and valid. We'll then use these indexes to filter data to divide it into outliers and valid samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VblL63efYhza",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "359340d7-0272-453b-c594-d1b712e24279"
      },
      "source": [
        "outliers = np.argwhere(kde_X < tau_kde)\n",
        "outliers = outliers.flatten()\n",
        "X_outliers = X[outliers]\n",
        "\n",
        "normal_samples = np.argwhere(kde_X >= tau_kde)\n",
        "normal_samples = normal_samples.flatten()\n",
        "X_valid = X[normal_samples]\n",
        "\n",
        "print(\"Original Samples : \",X.shape[0])\n",
        "print(\"Number of Outliers : \", len(outliers))\n",
        "print(\"Number of Normal Samples : \", len(normal_samples))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Samples :  500\n",
            "Number of Outliers :  25\n",
            "Number of Normal Samples :  475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 -Plot Outliers with Valid Samples for Comparison\n",
        "We have designed the method below named plot_outliers_with_valid_samples which takes as input valid samples and outliers and then plots them using different colors to differentiate between them. The figure will give a better idea about the performance of KernelDensity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_outliers_with_valid_samples(X_valid, X_outliers):\n",
        "    with plt.style.context((\"seaborn\", \"ggplot\")):\n",
        "        plt.scatter(X_valid[:, 0], X_valid[:, 1], c=\"tab:green\", label=\"Valid Samples\")\n",
        "        plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"tab:red\", label=\"Outliers\")\n",
        "        plt.legend(loc=\"best\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkXFMVZP76K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_outliers_with_valid_samples(X_valid, X_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WNB53OY8IfG",
        "colab_type": "text"
      },
      "source": [
        "#2 OneClassSVM \n",
        "\n",
        "The OneClassSVM estimator is available as a part of svm module of sklearn. It's based on the SVM algorithm which is used behind the scene to make a decision about the sample is outlier or not.\n",
        "\n",
        "Below is a list of important parameters of OneClasSVM which can be tweaked further to get better results:\n",
        "\n",
        " - kernel - It specifies the kernel type to be used for SVM. It accepts one of the below values as input.\n",
        "   - linear\n",
        "   - poly\n",
        "   - rbf\n",
        "   - sigmoid\n",
        "   - precomputed\n",
        " - degree - It accepts integer specifying degree of polynomial kernel (kernel='poly'). It's ignored when other kernels are used.\n",
        " - gamma - It specifies kernel coefficient to use for rbf, poly and sigmoid kernels. It accepts one of the below string or float as input.\n",
        "  - scale - It uses 1 / (n_features * X.var()) as value of gamma.\n",
        "  - auto - Default. It uses 1 / n_features as the value of gamma.\n",
        " - nu - It accepts float value in the range (0, 1] specifying upper bound on the fraction of training errors and lower bound on the fraction of support vectors.\n",
        " - cache_size - It specifies kernel cache size in MB. It accepts integer values as input. The default value is 200 MB. Itâ€™s recommended using more value for bigger datasets for better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR5OSQxWrk6B",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Fitting Model to DataÂ¶\n",
        "We'll now fit OneClassSVM to our Gaussian blobs dataset. We'll then use the trained model to make predictions about samples to let us know whether the sample is an outlier or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ykAt_v8egH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "nu = 0.05  # theory says it should be an upper bound of the fraction of outliers\n",
        "ocsvm = OneClassSVM(kernel='rbf', gamma=0.05, nu=nu)\n",
        "ocsvm.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdpTOYOlCAdP",
        "colab_type": "text"
      },
      "source": [
        "## 2.2-Predict Sample Class (Outlier vs Normal)Â¶\n",
        "\n",
        "OneClassSVM provides predict() method which accepts samples and returns array consisting of values 1 or -1. Here 1 represents a valid sample and -1 represents an outlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uHKjQWZ0WV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d80ea458-8b59-4818-9add-9b1a9182c656"
      },
      "source": [
        "preds = ocsvm.predict(X)\n",
        "preds[:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, -1,  1,  1,  1,  1, -1,  1,  1,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTe0Qqi3CAeA",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 - Dividing Dataset into Valid Samples and Outliers\n",
        "\n",
        "We'll now filter original data and divide it into two categories.\n",
        "\n",
        "- Valid Samples\n",
        "= Outliers\n",
        "We'll also print the size of samples that were considered outliers by model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4Ho1l8Ksjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_outliers = X[preds == -1]\n",
        "X_valid = X[preds != -1]\n",
        "\n",
        "print(\"Original Samples : \",X.shape[0])\n",
        "print(\"Number of Outliers : \", X_outliers.shape[0])\n",
        "print(\"Number of Normal Samples : \", X_valid.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwPfoQSS4Av",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Plot Outliers with Valid Samples for Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bB5XBn5tOFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_outliers_with_valid_samples(X_valid, X_outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZx-6XeziR0",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Important Attributes and methods of OneClassSVM\n",
        "Below is a list of important attributes and methods of OneClassSVM which can be used once the model is trained to get meaningful insights.\n",
        "\n",
        "- support_ - It returns indices of support vectors.\n",
        "- support_vectors_ - It returns actual support vectors of SVM.\n",
        "dual_coef_ - It represents coefficients of support vectors in decision function.\n",
        "- coef_ - It returns an array of the same size as that of features in dataset representing weights assigned to each feature. It works only when kernel linear is used.\n",
        "- intercept_ - It returns single float value representing intercept when using linear kernel.\n",
        "- decision_function(X) - It accepts dataset as input and returns signed distance for each sample of data. If the distance is positive then the sample is valid and outlier if negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q87nKz4zxr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Support Indices : \",ocsvm.support_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPUBga2Atw83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Support Vector Sizes : \", ocsvm.support_vectors_.shape)\n",
        "ocsvm.support_vectors_[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sgDFaxjt9lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Dual Coef Size \", ocsvm.dual_coef_.shape)\n",
        "ocsvm.dual_coef_[0][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyTJvAmAuL11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ocsvm_X = ocsvm.decision_function(X)\n",
        "\n",
        "X_outliers = X[ocsvm_X < 0]\n",
        "X_valid = X[ocsvm_X > 0]\n",
        "\n",
        "print(\"Number of Outliers : \", X_outliers.shape[0])\n",
        "print(\"Number of Normal Samples : \", X_valid.shape[0])\n",
        "\n",
        "ocsvm_X[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYtLLnosvWLU",
        "colab_type": "text"
      },
      "source": [
        "We can notice from the above output that decision_function() can be used to find out outliers as well and it'll return the same indexes as predict() for samples which are outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sq0XpVN0UZ7",
        "colab_type": "text"
      },
      "source": [
        "## 2.6-Trying OneClassSVM on DIGITS Dataset.\n",
        "We are now trying OneClassSVM on the digits dataset. We'll fit it to digits data and then use it to predict whether a sample is an outlier or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFeG8p_Sv2JX",
        "colab_type": "text"
      },
      "source": [
        "## 2.7 Fitting Model to Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2nyHRV0dnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " nu = 0.05  # theory says it should be an upper bound of the fraction of outliers\n",
        "ocsvm = OneClassSVM(kernel='rbf', gamma=0.05, nu=nu)\n",
        "ocsvm.fit(X_digits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUnd-y071Epv",
        "colab_type": "text"
      },
      "source": [
        "### 5.2- Plotting Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsBaFnzd1U_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, gaussian_nb.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Gq2A7BCAeX",
        "colab_type": "text"
      },
      "source": [
        "### 5.3 -Important Attributes of GaussianNB ###\n",
        "Below are list of important attributes available through estimator instance of GaussianNB.\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - epsilon_ - It represents absolute additive value to variances.\n",
        "  - sigma_ - It represents variance of each feature per class. (n_classes x n_features)\n",
        "  - theta_ - It represents mean of feature per class. (n_classes x n_features)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZKdf9heXoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gaussian_nb.class_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVgD1Ak6ed2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gaussian_nb.epsilon_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21fxEwfdekt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Gaussian Naive Bayes Sigma Shape : \", gaussian_nb.sigma_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS5rUYkYesAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Gaussian Naive Bayes Theta Shape : \", gaussian_nb.theta_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCpBVwViJPO",
        "colab_type": "text"
      },
      "source": [
        "## 6. ComplementNB \n",
        "The first estimator that we'll be introducing is ComplementNB available with the naive_bayes module of sklearn. We'll be first fitting it with default parameters to data and then will try to improve its performance by doing hyperparameter tuning. We'll also evaluate its performance using a confusion matrix. We'll even inform you regarding important attributes of ComplementNB which can give helpful insight once the model is trained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6as5LRNgjTng",
        "colab_type": "text"
      },
      "source": [
        "### 6.2 Fitting Model To Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld1_nNE_mDcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "complement_nb = ComplementNB()\n",
        "complement_nb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yN3QOkmDCAed",
        "colab_type": "text"
      },
      "source": [
        "### 6.3 - Evaluating Trained Model On Test Data.###\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMoGy7D4KXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Y_preds = complement_nb.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "\n",
        "print('Test Accuracy : %.3f'%complement_nb.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%complement_nb.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO6HAiq04ZKr",
        "colab_type": "text"
      },
      "source": [
        "### 6.4 Plotting Confusion MatrixÂ¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woRGjqgH4lTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, complement_nb.predict(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xH8Qq6J72il",
        "colab_type": "text"
      },
      "source": [
        "###6.5 -Important Attributes of ComplementNBÂ¶\n",
        "\n",
        "Below are list of important attributes available through estimator instance of ComplementNB.\n",
        "\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - feature_log_prob_ - It represents log probability of particular feature      based on class. (n_classes x n_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn6-jer48jeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complement_nb.class_log_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcudcn3rgf7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Log Probability of Each Feature per class : \", complement_nb.feature_log_prob_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J75Rv0fr8qWA",
        "colab_type": "text"
      },
      "source": [
        "### 6.6 Finetuning Model By Doing Grid Search On Various Hyperparameters.\n",
        "Below is a list of common hyperparameters that needs tuning for getting best fit for our data. We'll try various hyperparameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n",
        "\n",
        "  - alpha - It accepts float value representing the additive smoothing parameter. The value of 0.0 represents no smoothing. The default value of this parameter is 1.0.\n",
        "  \n",
        "We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDoYPVl48x_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],\n",
        "         }\n",
        "\n",
        "complement_nb_grid = GridSearchCV(ComplementNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
        "complement_nb_grid.fit(X_digits,Y_digits)\n",
        "\n",
        "print('Train Accuracy : %.3f'%complement_nb_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%complement_nb_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%complement_nb_grid.best_score_)\n",
        "print('Best Parameters : ',complement_nb_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Ubl4dFR1IG",
        "colab_type": "text"
      },
      "source": [
        "###6.6 Plotting Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UlaNus5S00W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, complement_nb_grid.best_estimator_.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2MOQ4EQSOR3",
        "colab_type": "text"
      },
      "source": [
        "##7.MultinomialNB \n",
        "The first estimator that we'll be introducing is MultinomialNB available with the naive_bayes module of sklearn. We'll be first fitting it with default parameters to data and then will try to improve its performance by doing hyperparameter tuning. We'll also evaluate its performance using a confusion matrix. We'll even inform you regarding important attributes of MultinomialNB which can give helpful insight once the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH3RPskMSFz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, complement_nb_grid.best_estimator_.predict(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Bpx7DCS_Hg",
        "colab_type": "text"
      },
      "source": [
        "###7.1 Fitting Default Model To Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRZyu778bBmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinomial_nb = MultinomialNB()\n",
        "multinomial_nb.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na4QkhBbbKUT",
        "colab_type": "text"
      },
      "source": [
        "###7.2 Evaluating Trained Model On Test Data\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict target variable on Test Set passed to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4jiDFJnbJAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = multinomial_nb.predict(X_test)\n",
        "\n",
        "print(Y_preds[:15])\n",
        "print(Y_test[:15])\n",
        "\n",
        "print('Test Accuracy : %.3f'%multinomial_nb.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models.\n",
        "print('Training Accuracy : %.3f'%multinomial_nb.score(X_train, Y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZfpXMFLbiDQ",
        "colab_type": "text"
      },
      "source": [
        "###7.3 Plotting Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXpM8bIebn7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, multinomial_nb.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkHO3kcibyVR",
        "colab_type": "text"
      },
      "source": [
        "###7.4 Important Attributes of MultinomialNB\n",
        "Below are list of important attributes available through estimator instance of MultinomialNB.\n",
        "\n",
        "  - class_log_prior_ - It represents log probability of each class.\n",
        "  - feature_log_prob_ - It represents log probability of particular feature based on class. (n_classes x n_features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJthSM_lcDZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multinomial_nb.class_log_prior_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3NMSeXacQf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Log Probability of Each Feature per class : \", multinomial_nb.feature_log_prob_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOU8Vi0BccAJ",
        "colab_type": "text"
      },
      "source": [
        "###7.5 Finetuning Model By Doing Grid Search On Various Hyperparame\n",
        "Below is a list of common hyperparameters that needs tuning for getting best fit for our data. We'll try various hyperparameters settings to various splits of train/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n",
        "\n",
        "  - alpha - It accepts float value representing the additive smoothing parameter. The value of 0.0 represents no smoothing. The default value of this parameter is 1.0.\n",
        "  \n",
        "We'll below try various values for the above-mentioned hyperparameters to find the best estimator for our dataset by splitting data into 3-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH7k634FcuyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ],\n",
        "         }\n",
        "\n",
        "multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
        "multinomial_nb_grid.fit(X_digits,Y_digits)\n",
        "\n",
        "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
        "print('Best Parameters : ',multinomial_nb_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpo2iWxdBaY",
        "colab_type": "text"
      },
      "source": [
        "###7.6 Plotting Confusion Matrix\n",
        "Below we are plotting the confusion matrix again with the best estimator that we found out using grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPRXvkRfdHcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(Y_test, multinomial_nb_grid.best_estimator_.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "Scikit-Learn - Anomaly Detection [Outliers Detection]\n",
        "https://coderzcolumn.com/tutorials/machine-learning/scikit-learn-sklearn-anomaly-detection-outliers-detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvqfD4BCAei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}