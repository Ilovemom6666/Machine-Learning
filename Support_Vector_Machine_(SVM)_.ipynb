{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Support Vector Machine (SVM) .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Machine-Learning/blob/master/Support_Vector_Machine_(SVM)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine (SVM) \n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "SVMs are the most popular algorithm for classification in machine learning algorithms. Their mathematical background is quintessential in building the foundational block for the geometrical distinction between the two classes.[1] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-4wqMJiHrUI",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=17EEgUZ0z3lMOLOZoU4BS2EY-iIOgEBpI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-HyuLjXeIsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Machine-Learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNfknPsvHnCj",
        "colab_type": "text"
      },
      "source": [
        "**What is SVM?**\n",
        "\n",
        "Support Vector Machines are a type of supervised machine learning algorithm that provides analysis of data for classification and regression analysis. While they can be used for regression, SVM is mostly used for classification. We carry out plotting in the n-dimensional space. Value of each feature is also the value of the specific coordinate. Then, we find the ideal hyperplane that differentiates between the two classes.\n",
        "\n",
        "These support vectors are the coordinate representations of individual observation. It is a frontier method for segregating the two classes [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVqWuonLIj6r",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1cWAgdofFTVDzY5GQl9X8zp14ZnS13Ol8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1l20QuMuqa",
        "colab_type": "text"
      },
      "source": [
        "**How does SVM work?**\n",
        "\n",
        "The basic principle behind the working of Support vector machines is simple – Create a hyperplane that separates the dataset into classes. Let us start with a sample problem. Suppose that for a given dataset, you have to classify red triangles from blue circles. Your goal is to create a line that classifies the data into two classes, creating a distinction between red triangles and blue circles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QegSNjgYNha7",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1i-PtkBz_aGEsSNJhcfvIzrJnrriRFgqK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMgYtkfENm9r",
        "colab_type": "text"
      },
      "source": [
        "While one can hypothesize a clear line that separates the two classes, there can be many lines that can do this job. Therefore, there is not a single line that you can agree on which can perform this task. Let us visualize some of the lines that can differentiate between the two classes as follows –"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFzcRm2BN1vb",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1FJcF0cs2fDfINXZ3uAJ-tYxaKJ5EOqBz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrPcV9JlOVya",
        "colab_type": "text"
      },
      "source": [
        "In the above visualizations, we have a green line and a red line. Which one do you think would better differentiate the data into two classes? If you choose the red line, then it is the ideal line that partitions the two classes properly. However, we still have not concretized the fact that it is the universal line that would classify our data most efficiently.\n",
        "\n",
        "The green line cannot be the ideal line as it lies too close to the red class. Therefore, it does not provide a proper generalization which is our end goal.\n",
        "\n",
        "According to SVM, we have to find the points that lie closest to both the classes. These points are known as **support vectors**. In the next step, we find the proximity between our dividing plane and the support vectors. The distance between the points and the dividing line is known as **margin**. The aim of an SVM algorithm is to maximize this very margin. When the margin reaches its maximum, the hyperplane becomes the optimal one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReDXdcwZPYua",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1M8XNNcRVa0eVsmzEWZyYsYg9qYRZXU3F)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvVgcrNlRPdM",
        "colab_type": "text"
      },
      "source": [
        "The SVM model tries to enlarge the distance between the two classes by creating a well-defined decision boundary. In the above case, our hyperplane divided the data. While our data was in 2 dimensions, the hyperplane was of 1 dimension. For higher dimensions, say, an n-dimensional Euclidean Space, we have an n-1 dimensional subset that divides the space into two disconnected components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_l24wD5RU2t",
        "colab_type": "text"
      },
      "source": [
        "#**2-How to implement SVM in Python?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "## **2.1 - Importing necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "005c1f49-042f-4282-dc0d-af00de6f481d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np                            #DataFlair\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%pylab inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "## **2.2 - Load Datasets**\n",
        "In the second step of implementation of SVM in Python, we will use the iris dataset that is available with the load_iris() method. We will only make use of the petal length and width in this analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQphjbMJSL34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pylab.rcParams['figure.figsize'] = (10, 6)\n",
        "iris_data = datasets.load_iris()\n",
        "# We'll use the petal length and width only for this analysis\n",
        "X = iris_data.data[:, [2, 3]]\n",
        "y = iris_data.target\n",
        "# Input the iris data into the pandas dataframe\n",
        "iris_dataframe = pd.DataFrame(iris_data.data[:, [2, 3]],\n",
        "                  columns=iris_data.feature_names[2:])\n",
        "# View the first 5 rows of the data\n",
        "print(iris_dataframe.head())\n",
        "# Print the unique labels of the dataset\n",
        "print('\\n' + 'Unique Labels contained in this data are '\n",
        "     + str(np.unique(y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 - Splitting Data Into Train/Test Sets**\n",
        "In the next step, we will split our data into training and test set using the train_test_split() function as follows –"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z8pZcxWTWes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "print('The training set contains {} samples and the test set contains {} samples'.format(X_train.shape[0], X_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_LkZDbxvRNM",
        "colab_type": "text"
      },
      "source": [
        "## **2.4 - Visualizing Data**\n",
        "Let us now visualize our data. We observe that one of the classes is linearly separable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RmDFIQlv7B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "markers = ('x', 's', 'o')\n",
        "colors = ('red', 'blue', 'green')\n",
        "cmap = ListedColormap(colors[:len(np.unique(y_test))])\n",
        "for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "           c=cmap(idx), marker=markers[idx], label=cl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zio5iZsm5-f6",
        "colab_type": "text"
      },
      "source": [
        "## **2.5-Data Scalling**\n",
        "Then, we will perform scaling on our data. Scaling will ensure that all of our data-values lie on a common range such that there are no extreme values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BaUQqe16TDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standard_scaler = StandardScaler()\n",
        "#DataFlair\n",
        "standard_scaler.fit(X_train)\n",
        "X_train_standard = standard_scaler.transform(X_train)\n",
        "X_test_standard = standard_scaler.transform(X_test)\n",
        "print('The first five rows after standardisation look like this:\\n')\n",
        "print(pd.DataFrame(X_train_standard, columns=iris_dataframe.columns).head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "##**2.6- Fitting SVM Model**\n",
        "After we have pre-processed our data, the next step is the implementation of the SVM model as follows. We will make use of the SVC function provided to us by the sklearn library. In this instance, we will select our kernel as ‘rbf’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DataFlair\n",
        "SVM = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
        "SVM.fit(X_train_standard, y_train)\n",
        "print('Accuracy of our SVM model on the training data is {:.2f} out of 1'.format(SVM.score(X_train_standard, y_train)))\n",
        "print('Accuracy of our SVM model on the test data is {:.2f} out of 1'.format(SVM.score(X_test_standard, y_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "## 7-Evaluating Trained Model On Test Data ##\n",
        "Almost all models in Scikit-Learn API provides predict() method which can be used to predict the target variables on Test Set passed to it. Most of the models also provide score() method which generally returns accuracy in the case of classification models. We'll utilize both methods below to compare results on test data.\n",
        "\n",
        "The majority of classifiers in scikit-learn also provide the predict_proba() method which can be used to see probability generated by the model for each class of classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Y_preds = classifier.predict(X_test)\n",
        "print(Y_preds)\n",
        "print(Y_test)\n",
        "print('Accuracy : %.3f'%(Y_preds == Y_test).mean() )\n",
        "print('Accuracy : %.3f'%classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqzO3cJd7ptl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classifier.predict_proba(X_test)[:10])  ## It returns probability predicted by model for each class for each example."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axCFNkj974Wc",
        "colab_type": "text"
      },
      "source": [
        "As we discussed above, logistic regression tries to generate lines through data to separate classes. We can access coordinates of those lines through coef_ and intercept_ attributes of classifier. In the case of binary classification, only 1 line separating both classes is generated. But in our case which consists of 3 classes, there are 3 lines generated separating each class from the other 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXd-wuZd8MH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Weight Coefficients : '+str(classifier.coef_))\n",
        "print('Y-axis Intercept : '+str(classifier.intercept_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "## 8 -Visualizing Prediction Results On Test Data ##\n",
        "\n",
        "Below we are trying to visualize how our model performed on test data by plotting scatter chart of sepal length vs petal width and color-encoding them with flower class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(('ggplot','seaborn')):\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(121)\n",
        "    for i,c in [(0,'red'),(1,'green'),(2,'blue')]:\n",
        "        plt.scatter(X_test[Y_test==i,0],X_test[Y_test==i,3], c=c, s=40, marker='s', label=iris.target_names[i])\n",
        "    plt.xlabel(iris.feature_names[0])\n",
        "    plt.ylabel(iris.feature_names[3])\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Actual')\n",
        "\n",
        "    plt.subplot(122)\n",
        "    for i,c in [(0,'red'),(1,'green'),(2,'blue')]:\n",
        "        plt.scatter(X_test[Y_preds==i,0],X_test[Y_preds==i,3], c=c, s=40, marker='s', label=iris.target_names[i])\n",
        "    plt.xlabel(iris.feature_names[0])\n",
        "    plt.ylabel(iris.feature_names[3])\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Prediction');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdpTOYOlCAdP",
        "colab_type": "text"
      },
      "source": [
        "## 9- Finetuning Model By Doing Grid Search On Various Hyperparameters\n",
        "\n",
        "Below are list of hypterparameters that we can tune to get best estimator for our data. \n",
        "1. penalty - Penalty to be used in model to penalize weights to avoid over-fitting and under-fitting. It accepts string like l1, l2, elasticnet, and none. elasticnet refers to using both l1 and l2 in proportion. default=l2\n",
        "\n",
        "2. fit_intercept - It's boolean value referring whether to include intercept in model or not ( y=mx+c  - here c is referring to intercept).default=True\n",
        "3. C - It's inverse of regularization strength(1/ α  whereas  α  is regularization strength in our cost function). default=1.0\n",
        "4. solver - Algorithms for optimization. It accepts string from list ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] default=liblinear \n",
        "5. l1_ratio - When penalty is elasticnet then this parameter helps in determining proportion of l1 & l2 penalties. It accepts float(0.0-1.0] or None value. l1_ratio=0 is equivalent to using penalty=l2. l1_ratio=1 is equivalent to using penalty=l1. default=None\n",
        "**GridSearchCV**\n",
        "\n",
        "It's a wrapper class provided by sklearn which loops through all parameters provided as params_grid parameter with a number of cross-validation folds provided as cv parameter, evaluates model performance on all combinations and stores all results in cv_results_ attribute. It also stores model which performs best in all cross-validation folds in best_estimator_ attribute and best score in best_score_ attribute.\n",
        "\n",
        "Note: n_jobs parameter is provided by many estimators. It accepts a number of cores to use for parallelization. If the value of -1 is given then it uses all cores. We are also using %%time which jupyter notebook cell magic command which prints time taken by that cell to complete running. Time will be different on different computers based on their configurations.\n",
        "\n",
        "Below we are trying liblinear solver for our purpose. We can only use penalties l2, l1 with this algorithm. It works faster for small datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51KqENlj_e2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e6002433-4678-4d87-ff2f-406bc373c7db"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'penalty' : ['l1', 'l2',],\n",
        "         'fit_intercept': [True, False],\n",
        "         'C': np.linspace(0.1,1.0,10)}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(random_state=1, n_jobs=-1), param_grid=params, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.975\n",
            "Test Accuracy : 0.967\n",
            "Best Score Through Grid Search : 0.975\n",
            "Best Parameters :  {'C': 0.4, 'fit_intercept': False, 'penalty': 'l2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTe0Qqi3CAeA",
        "colab_type": "text"
      },
      "source": [
        "## 10 - Printing First Few Cross-Validation Results##\n",
        "GridSearchCV object maintains all different parameters tried and results generated for each split of data in an attribute cv_results_ as a dictionary. Below we are loading that cross-validation results as pandas dataframe and printing first few entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm-21jozCAeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXa4ttgczPfN",
        "colab_type": "text"
      },
      "source": [
        "Below we are trying saga solver for our purpose. We can only use penalties l2, l1, elasticnet or no penalty(none) with this algorithm. It's the only algorithm which supports elasticnet penalty. It works faster for large datasets.# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNgIRP9ezR5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'penalty' : ['l1', 'l2','elasticnet', 'none'],\n",
        "         'fit_intercept': [True, False],\n",
        "         'C': np.linspace(0.1,1.0,10),\n",
        "         'l1_ratio': np.linspace(0.1,1.0,10)}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(random_state=1, solver='saga', n_jobs=-1), param_grid=params, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrZx-6XeziR0",
        "colab_type": "text"
      },
      "source": [
        "**Printing First Few Cross Validation Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q87nKz4zxr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEmkNdqh0IF7",
        "colab_type": "text"
      },
      "source": [
        "Below we are trying sag solver for our purpose. We can only use penalty l2 or no penalty(none) with this algorithm. It works faster for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmvm9kmy0MUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'penalty' : ['l2', 'none'],\n",
        "         'fit_intercept': [True, False],\n",
        "         'C': np.linspace(0.1,1.0,10),\n",
        "         'l1_ratio': np.linspace(0.1,1.0,10)}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(random_state=1, solver='sag', n_jobs=-1), param_grid=params, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sq0XpVN0UZ7",
        "colab_type": "text"
      },
      "source": [
        "**Printing First Few Cross Validation Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ2nyHRV0dnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuzUSRhS00EA",
        "colab_type": "text"
      },
      "source": [
        "Below we are trying lbfgs solver for our purpose. We can only use penalty l2 or no penalty(none) with this algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdMC_1w607mF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'penalty' : ['l2','none'],\n",
        "         'fit_intercept': [True, False],\n",
        "         'C': np.linspace(0.1,1.0,10),\n",
        "         'l1_ratio': np.linspace(0.1,1.0,10)}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(random_state=1, solver='lbfgs', n_jobs=-1), param_grid=params, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUnd-y071Epv",
        "colab_type": "text"
      },
      "source": [
        "**Printing First Few Cross Validation Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsBaFnzd1U_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcZ4fX2j1fcs",
        "colab_type": "text"
      },
      "source": [
        "Below we are trying newton-cg solver for our purpose. We can only use penalty l2 or no penalty(none) with this algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf-y4XBE1jTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7734bad0-4ceb-4fb3-8c51-9b12166a8654"
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'penalty' : ['l2','none'],\n",
        "         'fit_intercept': [True, False],\n",
        "         'C': np.linspace(0.1,1.0,10),\n",
        "         'l1_ratio': np.linspace(0.1,1.0,10)}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(random_state=1, solver='newton-cg', n_jobs=-1), param_grid=params, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy : 0.975\n",
            "Test Accuracy : 0.967\n",
            "Best Score Through Grid Search : 0.975\n",
            "Best Parameters :  {'C': 0.4, 'fit_intercept': False, 'l1_ratio': 0.1, 'penalty': 'l2'}\n",
            "CPU times: user 2.01 s, sys: 80.2 ms, total: 2.09 s\n",
            "Wall time: 1min 2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1501: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  \"(penalty={})\".format(self.penalty))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0jjIpLL1rqe",
        "colab_type": "text"
      },
      "source": [
        "**Printing First Few Cross Validation Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nbzyTxc12R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Gq2A7BCAeX",
        "colab_type": "text"
      },
      "source": [
        "## 11 - K-Nearest Neighbors##\n",
        "\n",
        " K-nearest neighbor is one of the simplest algorithms which maintains all points from the train dataset and class to which it belongs. Later on, whenever a new unknown point comes for prediction it checks a predefined number of points nearer to that new point and based on majority class it assigns that majority class to a new point.n_neighbors is used to set the number of neighbors to check for predicting class for new unseen points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCpBVwViJPO",
        "colab_type": "text"
      },
      "source": [
        "### 11.1 Initializing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKE53swWipAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "knn_classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6as5LRNgjTng",
        "colab_type": "text"
      },
      "source": [
        "### 11.2 Fitting Model To Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld1_nNE_mDcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn_classifier.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yN3QOkmDCAed",
        "colab_type": "text"
      },
      "source": [
        "### 11.3 - Evaluating Trained Model On Test Data.###\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMoGy7D4KXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_preds = knn_classifier.predict(X_test)\n",
        "print(Y_preds)\n",
        "print(Y_test)\n",
        "print('Accuracy : %.3f'%(Y_preds == Y_test).mean())\n",
        "print('Accuracy : %.3f'%knn_classifier.score(X_test, Y_test)) ## Score method also evaluates accuracy for classification models."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhsVrbTu4SqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(knn_classifier.predict_proba(X_test)[:10]) ## It returns probability predicted by model for each class for each example."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO6HAiq04ZKr",
        "colab_type": "text"
      },
      "source": [
        "### 11.4 Visualizing Prediction Results On Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woRGjqgH4lTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with plt.style.context(('ggplot','seaborn')):\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(121)\n",
        "    for i,c in [(0,'red'),(1,'green'),(2,'blue')]:\n",
        "        plt.scatter(X_test[Y_test==i,0],X_test[Y_test==i,3], c=c, s=40, marker='s', label=iris.target_names[i])\n",
        "    plt.xlabel(iris.feature_names[0])\n",
        "    plt.ylabel(iris.feature_names[3])\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Actual')\n",
        "\n",
        "    plt.subplot(122)\n",
        "    for i,c in [(0,'red'),(1,'green'),(2,'blue')]:\n",
        "        plt.scatter(X_test[Y_preds==i,0],X_test[Y_preds==i,3], c=c, s=40, marker='s', label=iris.target_names[i])\n",
        "    plt.xlabel(iris.feature_names[0])\n",
        "    plt.ylabel(iris.feature_names[3])\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Prediction');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xH8Qq6J72il",
        "colab_type": "text"
      },
      "source": [
        "###11.5 -Finetuning Model By Doing Grid Search On Various Hyperparameters.#\n",
        "Below are list of hypterparameters that we can tune to get best estimator for our data.\n",
        "\n",
        "**n_neighbors** - Number of neighbors to use to determine class of target. default=5\n",
        "\n",
        "**algorithm** - Algorithm for finding nearest neighbors. It takes one of the values from list [ball_tree, kd_tree, brute, auto]. default=auto\n",
        "\n",
        "**leaf_size** - Leaf size of KDTree and BallTree. It controls speed of construction and quer of tree as well as memory requirement of tree.default=30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn6-jer48jeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "params = {'n_neighbors' : np.arange(1,10),\n",
        "         'leaf_size': np.arange(5,50,5),\n",
        "         'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto']}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(n_jobs=-1), param_grid=params, cv=3, n_jobs=-1)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print('Train Accuracy : %.3f'%grid.best_estimator_.score(X_train, Y_train))\n",
        "print('Test Accuracy : %.3f'%grid.best_estimator_.score(X_test, Y_test))\n",
        "print('Best Score Through Grid Search : %.3f'%grid.best_score_)\n",
        "print('Best Parameters : ',grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J75Rv0fr8qWA",
        "colab_type": "text"
      },
      "source": [
        "### 11.6 Printing First Few Cross Validation Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDoYPVl48x_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_results = pd.DataFrame(grid.cv_results_)\n",
        "print('Number of Various Combinations of Parameters Tried : %d'%len(cross_val_results))\n",
        "cross_val_results.head() ## Printing first few results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvqfD4BCAei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38xv6OCnGaTV",
        "colab_type": "text"
      },
      "source": [
        "# **Refences**\n",
        "[1] Support Vector Machines Tutorial\n",
        "\n",
        "https://data-flair.training/blogs/svm-support-vector-machine-tutorial/?fbclid=IwAR0WAHSGp4wFaVpT38IfpQXsHTgSzM8ziTkrjaXGQtzAPmbQy9oMcDjrRvE\n",
        "\n",
        "How to insert an inline image in Google Colaboratory from Google Drive\n",
        "\n",
        "https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive"
      ]
    }
  ]
}